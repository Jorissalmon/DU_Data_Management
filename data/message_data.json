[
    "Hi all,I just wanted to share a post I made regarding how to create and submit a simple model for this competition.This is really for people who might be new to machine learning and are looking for a starting point. The tutorial walks through the steps in order to create a basic model, using R. I am relatively new to ML myself, and learned quite a lot from the Titanic tutorials that are available, so I tried to create something along those lines for the Bike Sharing competition.I hope you find it useful!brandonharris.io - A Simple Model for Kaggle Bike Sharing Simple yet effective preprocessing. Nice! Thanks, Brandon. Very helpful article.I created a plot to visualize some of the trends you found in the datetime variable.The darker green shows \"more popular\" bike rental times. Thanks. I'm learning from the Analytics Edge course on edX.Here's the general idea (using R and ggplot2 package):# pull the weekday and hour from the datetime variable with R's strptime() functiontrain$datetime <- strptime(train$datetime, format=\"%Y-%m-%d %H:%M:%S\")train$weekday <- weekdays(train$datetime)train$hour <- train$datetime$hour# Save average counts for each day/time in data frameday_hour_counts <- as.data.frame(aggregate(train[,\"count\"], list(train$weekday, train$hour), mean))day_hour_counts$Group.1 <- factor(day_hour_counts$Group.1, ordered=TRUE, levels=c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))day_hour_counts$hour <- as.numeric(as.character(day_hour_counts$Group.2))# plot heat mat with ggplotlibrary(ggplot2)ggplot(day_hour_counts, aes(x = hour, y = Group.1)) + geom_tile(aes(fill = x)) + scale_fill_gradient(name=\"Average Counts\", low=\"white\", high=\"green\") + theme(axis.title.y = element_blank()) HI,Inspired by this thread andBeata Strubelvisualization I built a shiny app for visualizing the dataset. It does not support all filters yet, but it might be useful to some of you as it is :).Here is the link: https://mlespiau.shinyapps.io/devdataprod-016/ Hello,Since this topic has a simple solution using R I would like to add my simple solution using Python.Not everyone knows R and some of us need help getting started using Python, so I would like to share my solution which is similar to @nameBrandon. I am not an expert myself, however it might help someone who hasn't had a lot of experience using Pandas and Scikit-learn.Open to suggestions.Thankshttp://nbviewer.ipython.org/github/gig1/Python_Kaggle_Byke_Sharing_Demand/blob/master/Bicycle%20Tutorial.ipynb Thanks for the code Brandon. I think there is a mistake in the code on github. When you check for Sunday your code is:--------#create Sunday variabletrain_factor$sunday[train_factor$day == \"Sunday\"] <- \"1\"train_factor$sunday[train_factor$day != \"1\"] <- \"0\"test_factor$sunday[test_factor$day == \"Sunday\"] <- \"1\"test_factor$sunday[test_factor$day != \"1\"] <- \"0\"-------But this sets all the values to 0. What you want is:-------#create Sunday variabletrain_factor$sunday[train_factor$day == \"Sunday\"] <- \"1\"train_factor$sunday[train_factor$day != \"Sunday\"] <- \"0\"test_factor$sunday[test_factor$day == \"Sunday\"] <- \"1\"test_factor$sunday[test_factor$day != \"Sunday\"] <- \"0\"------- Thanks Beata Strubel and Brandon!Using the thermal plot we can see another trend. Casual bikers are more likely to rent on weekends than Registered bikers (Count=Casual+Registered btw). I have exhausted my submissions today so will check tomorrow how this makes a difference.I am thinking of running two models separately. One for casual and one for registered and adding them. What do you guys think? Is this better/ worse than including the variable (casual-registred) in Brandon's model? Several conclusions how to get result to top 10%try different output transformationstune your algo parameters and predictors used with cross validationmake ensembles of different models (the lower correlation the higher enhance)analyze your residuals with CV and make adjustments in case of systematic errorsHope this would help somebody. This is awesome! Interesting approach. Thanks, it was great reference Nice👍 Nice 👍👍👍 Thank you, Brandon, you give us a great lesson about feature engineering, it is very helpful! Nice article Sergey Makarevich wroteshanxin136 wroteSergey Makarevich wroteSeveral conclusions how to get result to top 10%try different output transformationstune your algo parameters and predictors used with cross validationmake ensembles of different models (the lower correlation the higher enhance)analyze your residuals with CV and make adjustments in case of systematic errorsHope this would help somebody.Could you give me an example (codes will be the best) about how to combine different models? I am really interested in ensemble methods.I used simple average that time. Which means you add your models predictions and divide the sum by number of models. So there is nothing to send.More advanced averaging is simply using your models outputs as inputs to linear regression which you fit to your necessary outcome (with cv of course).If you need my model - contact me via skype sergey_mmmI see. That is simple. Thanks! shanxin136 wroteSergey Makarevich wroteSeveral conclusions how to get result to top 10%try different output transformationstune your algo parameters and predictors used with cross validationmake ensembles of different models (the lower correlation the higher enhance)analyze your residuals with CV and make adjustments in case of systematic errorsHope this would help somebody.Could you give me an example (codes will be the best) about how to combine different models? I am really interested in ensemble methods.I used simple average that time. Which means you add your models predictions and divide the sum by number of models. So there is nothing to send.More advanced averaging is simply using your models outputs as inputs to linear regression which you fit to your necessary outcome (with cv of course).If you need my model - contact me via skype sergey_mmm Sergey Makarevich wroteSeveral conclusions how to get result to top 10%try different output transformationstune your algo parameters and predictors used with cross validationmake ensembles of different models (the lower correlation the higher enhance)analyze your residuals with CV and make adjustments in case of systematic errorsHope this would help somebody.Could you give me an example (codes will be the best) about how to combine different models? I am really interested in ensemble methods. Hi all,On my free time I try to learn few things lately i found myself interested in ML algorithms with Python and trying to understand how everything fits together...I have spent last couple weeks trying to come up with the best algorithm to predict this problem...form the forum it seems that RandomForestRegressor is the best Model...But it appear I'm not able to by pass the .57 score...Below is the link to the Model that I have come up with...Any feedback will be much appreciated. Thankshttp://htmlpreview.github.io/?https://github.com/mmlamari/RepoTest/blob/master/Bike_Sahring_Python.html Hi kagglers,For those who are solving the missing values imputation problem of this exercise, I would just like to mention 4 participations in this good introductory thread that I think should be taken as relevant ones. In descending order:4) The Hot Beata Strubel's Heat Map. For those who want to impress...3) Sergey Makarevich's Four Steps to Enlightenment.2) nameBrandon's EasyPeasy R code. The way of entry to \"value imputation\" problem (but be aware of some conceptual errors...).And the winner is....!1) gig1's \"How-to-start-setting-a-true-data-mining-study\" link. Excellent. He worked it in python, but those who know R should be doing something similar probably using caret... By this time, many of the assumptions have changed and new information about the problem is available but it should be easy to modify the code and include those changes.Success! Removing Sunday from the same model is boosting up the score to 0.49445. Can some one please give some suggestions on the rationale behind this ? ArjunDid you install the library 'party' and call it before using the cree function?Arjun Nair wroteI am getting an error:> fit.ctree <- ctree(formula, data=train_factor)Error: could not find function \"ctree\"Could someone help me with this. arolas wroteHI,Inspired by this thread andBeata Strubelvisualization I built a shiny app for visualizing the dataset. It does not support all filters yet, but it might be useful to some of you as it is :).Here is the link: https://mlespiau.shinyapps.io/devdataprod-016/Hey, i love this shiny app. I just recreated the code in R for grouping the data by month as well. but i can't figure out how to plot the data for each month in one diagramm? or even for each month in one step.does anyone know a solution? Wow fantastic article and thanks for sharing! Beata Stubel ...that is a nice plot.It gives complete understanding of the dataset.Thanks",
    "when it comes to the weather column It has values [1, 2, 3, 4]. What do they actually refer to? And should I convert it to categorical? If so, what categories should I convert these numbers to? Hi Omar! I haven't played around with this dataset, but based on the dataset description, each of the four numbers represents various types of weather:1: Clear, Few clouds, Partly cloudy, Partly cloudy2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + FogConverting the numbers 1 through 4 to a categorical dtype was my initial knee-jerk reaction, but I'm not sure what question(s) you are trying to answer with this data so you might want to provide a bit more context here in the discussion! Thanks I wasn't able to get these info about the data since I'm new to kaggle",
    "Looking at the visual, got a couple of weird things going on in both decembers. Blue being the traininig set, and Orange being the scored test set. I'll investigate further and report back. *This gets an rsmle of ~0.44.",
    "Just to share my learnings, please check me if you would like to discuss more details either on coding or modeling.1). GBR (GradientBoostRegressor) is slightly better than RFR (RandomForestRegression), both RMSLE ~ 0.42). A simple ANN or CNN is not better than GBR or RFR, RMSLE ~ 1.0.3). I put more efforts on RNN since its architecture seems better suited for the problem.  The initial run of RNN is similar (as bad as) to ANN/CNN, More hidden units and change activation function (Rulu/Tanh/Sigmoid) seems helpful.(reading from this link:https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/, it seems suggesting MLP prefer Relu, CNN prefer Relu, RNN prefer sigmoid and tanh.)4). I tried LSTM/GRU and did not see two models to be significant different.5). I tried time_steps from 7 days to 14 days and did not see significant difference.6). Including Test data in data preprocess step (data normalization) seems helpful.7). Including prediction of test data back to training dataset to generate new training samples and inject to model fitting seems helpful as well.8). Removing seasonality (especially the hourly periodicity)) did not help much.9). About 50 hidden units and 100 Epochs with a couple of iteration with above process seems can get RMSLE to 0.4 but still worse than GBR or RFR.10). Though ensemble method may help to boost RMSLE, I have not tried.  There should be many other tricks to boost RMSLE to below 0.4 such as increasing hidden units or epoches, and feature engineering. I believe my learning on RNN maybe diminishing thus I stopped here.11). Last thought is Bidirectional LSTM.  It seems its forward and backward is on the same sequence.  I was wondering whether we can built a LSTM model with two LSTM layer in parallel (one sequence forward and a separate different sequence backward) to predict the later 10 days of bike sharing demand of each month.",
    "After I used AutoGluon to train a model and when I want to see the fit summary it gave me all score values in minus as example listed below**model                             score_valWeightedEnsemble_L3  -50.674041**",
    "Looks like something's wrong with the data. Data shown and data getting downloaded are different. Besides, the train.csv contains 2 extra columns (highly correlated with Count column) than test.csv, which needs to be removed in order to run the data datasets Yes.. 'count' is actually sum of 'casual' and 'registered' column.",
    "temp_df.info()->RangeIndex: 10886 entries, 0 to 10885Data columns (total 21 columns):#   Column      Non-Null Count  Dtype---  ------      --------------  -----0   datetime    10886 non-null  object1   season      10886 non-null  int642   holiday     10886 non-null  int643   workingday  10886 non-null  int644   weather     10886 non-null  int645   temp        10886 non-null  float646   atemp       10886 non-null  float647   humidity    10886 non-null  int648   windspeed   10886 non-null  float649   casual      10886 non-null  int6410  registered  10886 non-null  int6411  count       10886 non-null  int6412  year        10886 non-null  int6413  date        10886 non-null  datetime64[ns]14  month       10886 non-null  int6415  day         10886 non-null  int6416  hour        10886 non-null  int6417  minutes     10886 non-null  object18  seconds     10886 non-null  object19  second      10886 non-null  int6420  minute      10886 non-null  int64temp_df['season'].eq(2)( 1 = spring, 2=summer, 3=fall, 4=winter  )->0        False1        False2        False3        False4        False…10881    False10882    False10883    False10884    False10885    FalseThis is what i tried.. but the results are not satisfactory. trytemp_df[temp_df['season'].eq(2)]ortemp_df[temp_df['season'] == 2]",
    "Hi,Does anyone know the reason why the errors below happen and how to fix it?Looks the format and number of rows (6493) are same as those in the sampleSubmission.csv file.ERROR: Could not parse '' into expected type of Double (Line 6502, Column 2)ERROR: The value '' in the key column 'datetime' has already been defined (Line 6503, Column 1)ERROR: Could not parse '' into expected type of Double (Line 6503, Column 2)ERROR: The value '' in the key column 'datetime' has already been defined (Line 6504, Column 1)ERROR: Could not parse '' into expected type of Double (Line 6504, Column 2)Thanks in advance! good! bro~!~ Problem is resolved by customizing the format of \"datetime\" column as yyyy-mm-dd hh:mm:ss.",
    "When I try to use LR to predict the result of casual value, and I wanna to get the RMSLE just suggested in this competition. I learned that I can use functionmean_squared_log_errorwithnp.sqrtto get the RMSLE, but when I try themean_suqared_log_error, I got an error withValueError:Mean Squared LogarithmicErrorcannot be usedwhentargets contain negative values.content_copyHow can I solve it? Some predictions may be having negative values, make all negative predictions to zero",
    "I am having trouble submitting my result. It seems my datatime format issue?1/20/2011 0:00    needs to convert 2011-01-208 0:00:00Have tried different ways, no luck.Can someone help? It will be better if you use submission csv at the end and use date column from this",
    "ERROR: Unable to find 867 required key values in the 'datetime' columnERROR: Unable to find the required key value '2011-01-20 01:00:00' in the 'datetime' columnERROR: Unable to find the required key value '2011-01-20 02:00:00' in the 'datetime' columnERROR: Unable to find the required key value '2011-01-20 12:00:00' in the 'datetime' columnERROR: Unable to find the required key value '2011-01-22 13:00:00' in the 'datetime' columnAnyone has problem like me and how to solve it ? I faced the same issue. After you implement the model and \"write\" the csv file out. go to that csv file and open it. Click on the datetime column, select \"format cells…\". In it, go to custom category and under the \"Sample\" bar type the required format which is \"yyyy-mm-dd hh:mm:ss\". Then submit it.Hope this helps.",
    "MAN18-2 Nikita Shikulin",
    "I'd like to share some findings and hear your critics/comments/etc.Note: I do not claim my statements to be true, just to be my opinion at the time of writing with my limited knowledge and comprehension.On the photos, I include the settings so you don't wonder what exactly you are looking at.Some hours have no data. They should be filled with zeros if you do mean/average calculationsBoth, casual and specially registered have a similar pattern each day of the weekThe weather affects the results, but the pattern is still thereThere are very few holidays. When they happen, the casual grows substantially. Not visible here, but the day before the holiday gets affected by the holiday tooBe careful as it is easy to fool yourself with the increase. We should compare side by side with non-holidays days in the same months only (temperature affects everything)In the next photo, you have 3 bumps (non-holiday-weekend non-holiday-workingday holiday-workingday). Here we compare the last 2 ones. We could see some people still go to work, but they move much more during the day.Casual, they do go out more, but shifting the peak hours, so as to attend whatever is going on on town in the morning (again, ignore the first bump)Days with holidaysWith this is easy to see where are the missing hours. There is a large gap of missing hours one morning. I think they had a failure in the recording of data (or the system was down not giving bikes).![]()![]()![]()![]()That's all for now.Tool used:https://github.com/nicolaskruchten/jupyter_pivottablejs(I love it, absolutely fantastic) I'd like to comment as well the excellent work done herehttps://github.com/logicalguess/kaggle-bike-sharing-demand/blob/master/code/main.pycreating features.He created a feature called \"peak\" that depends on the type of workday/weekend/holiday\"ideal\" that depends on temperature and wind\"sticky\" f(humidity) but only on workingdays (you don't want to arrive to work all sweaty)\"countseason\" which is a sum of count per seasonThe author also modified lots of holidays/workdaysTrain+Test modifiedTrain (original)![]()![]()",
    "Here is the link to my notebook -https://github.com/gauravbansal98/Bike-Sharing-Demand/blob/master/Bike%20Sharing%20Demand.ipynb",
    "in train data1,2,3 is spring 4,5,6 is summer 7,8,9 is autumn and 10,11,12 is winterbut real is 3,4,5 spring 6,7,8 summer 9,10,11 autumn and 12,1,2 winterwhat is wrong? we need to change data? I see 4 seasons.Kind of agree that they have the wrong label.But in my case it does not matter, they are just \"categories\".Do their individual label affect your model? How? There are only 4 season values in my data, but I would also argue that there is a failure … temperatures during spring are lower than winter (training data). So probably one should not think in the typical categories here.",
    "What does holiday mean here? is it a weekend holiday? Or it's annual festive like Christmas? And what does it mean when both workingday and holiday are 0? I think this link can answer your questionhttps://www.kaggle.com/c/bike-sharing-demand/discussion/9876you can consider the feature like this.\"Working Day\" (workingday=1, holiday=0)\"Holiday\" (workingday=0, holiday=1)\"Weekend\" (workingday=0, holiday=0) I guess when its not holiday and not working day, it could mean the weekday is a weekend… weekend - although depending on where you live the 01.01. might also be a holiday.",
    "Dear all,I tried to predict the number of bike rent using Support Vector Regression, PCR, and ridge but it produced several negative values. Any idea to help on how to solve this problem?The covariates I used were: season+ workingday+weather+ temp+ humidity+ windspeed +hour+ DayText+YearTest.Thank you very much for your help",
    "Help me",
    "I submitted it yesterday, but it won't be submitted after a day. So I can't submit a new one and do nothing. What should I do?",
    "Hello,I created a little tutorial on how to reach a reasonably good score using Rs randomForest package.By following the tutorial you will learn how to do basic data manipulation with R, perform basic Feature Engineering and use the randomForest package.The tutorial as well as the corresponding R file are attached to this post. It's way too long for a forum post and I don't have a blog. (If anyone wants to publish this on her or his blog, feel free to contact me anytime)Hope you enjoy this, let me know your feedback.EDIT: Changed some minor details in the Tutorial and added a section \"Validation\". Thanks for sharing your tutorial. I don't want to be a party-pooper but I think your method violates the competition rule, discussedhere. I had a similar approach to yours, but I just found out about it today, and now I have to scrap and re-do everything. :( Really good tutorial! Helped me a lot! Thank you @bruschkov Thanks!!! It is a great tutorial!! It is useful and easy to follow! Arjun Nair wroteI am getting an error:> casualFit <- randomForest(casual ~ hour + year + humidity + temp + atemp + workingday + weekday, data=train, ntree=myNtree, mtry=myMtry, importance=myImportance)Error in na.fail.default(list(casual = c(3L, 8L, 5L, 3L, 0L, 0L, 2L, 1L, :missing values in object> test$casual <- predict(casualFit, test)Error in predict(casualFit, test) : object 'casualFit' not found> registeredFit <- randomForest(registered ~ hour + year + season + weather + workingday + humidity + weekday + atemp, data=train, ntree=myNtree, mtry=myMtry, importance=myImportance)Error in na.fail.default(list(registered = c(13L, 32L, 27L, 10L, 1L, 1L, :missing values in object> test$registered <- predict(registeredFit, test)Error in predict(registeredFit, test) : object 'registeredFit' not foundCould Someone please clear this.Maybe the reason is that by default week days are defined this way in the code:df$weekday <- factor(df$weekday, levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\")) #order factorsYou should rename \"Montag\", \"Dienstag\",etc. according to the language of your system,e.g. \"Monday\",\"Tuesday\",.. Thanks for your feedback.Simple regression algorithms are in fact very efficient, especially for Big Data. The reason is that a simple regression is relatively easy to compute, easy to implement and thus results in a very performant model. If you want to predict vast amount of datasets at runtime, for example, a simple regression model would make a good choice over a relatively slow random forest. (Keep in mind that kaggle competitions are nowhere near Big Data. Big Data starts at a couple of Terabytes or so, which is too big for the working memory of most computer systems).I used a random Forest model simply because it gave me the best results and I am not very proficient with linear regressions. Surely someone proficient with regressions would be able to beat my random Forest score with a good regression model. I'm new to Kaggle doing some late practice submissions and this really helped a lot👍👍 Hola, alguien me podría decir dónde obtengo los ficheros .csv?   gracias @feelosophy13: Yes, I know. A second version is \"in the works\", but I am swamped with other things right now...@sujith kumar: The standard way to handle categorial variables in regression analysis is to break them up into binary \"dummy variables\".E.g. for the variable \"weekday\" you would create 7 new columns called \"Monday\", \"Tuesday\" and so forth. The new columns contain binary values, \"1\" if its a Monday, \"0\" if it's any other day of the week.The same should be done for hours of the day, month, year and every other categorial variabe.For additional info see here: http://en.wikipedia.org/wiki/Dummy_variable_(statistics)This R package might help: http://cran.r-project.org/web/packages/dummies/dummies.pdf Thats a very great tutorial, thanks for the tutorial sir..as we have categorical variables here like season, holiday ,workingday and weather.. how can we apply a  linear regression over here,  when we do apply so how can regression handle both categorical and numerical data.. in prediction?? Thanks for this nice tutorial! Thanks, that worked. Great tutorial Thanks for the suggestion.I'm currently working on a second model for this dataset based on a different approach. I might use the looping method in that one. To avoid violating rule with using all data for the model, why don't you use a loop by month and running your code for each month by using the first 20th day as train to predict the a end of the month. Thanks for your feedback! I will try and and incorporate your thoughts for my next tutorials (whenever I find time to write a new one...). Thank you for the very good tutorial! It helped me to get started with R and its random forest library.The structure is great, and you do a very good job also seeing the bigger picture (what to do with other problems than this example) and to mention further reading.For me, two things might have been improved. They are of minor importance and ordered in decreasing one:When talking about \"varImpPlot\", you could add the command in green like you do everywhere else:varImpPlot(testFit)For understanding the steps in the FeatureEngineer function, it might be a little better to do them step by step on a subset of the training set first, and to explain every step. Later, you can give the whole function in one step and apply it to train and test. I did so in particular to understand the part with[,names]because I am new to R.However, it is great and it helped me a lot. Thank you very much! Hi,I'm new to Kaggle and have been learning data analysis from this site.I can publish your tutorial on my blog if you want. Thanks! Hi,random forest and your tutorial are very useful for me :)Can I ask why using simple regression ( such as linear regression or stepwise etc) is not efficient for big data? many thanks for R codes.",
    "Hey,I just started working on Kaggle projects, anyone looking for a new team member. Hey, I would like to join. Hey, I would like to join as well! yes, i am also looking for a partner",
    "Hi everyone. Now that the competition is over I wonder if its now okay to share what we got? I haven't been able to do better than over 600 of you, so I would like to learn from what you guys did, if it is cool to do so, now. https://github.com/logicalguess/kaggle-bike-sharing-demand Good work 👌 Hi Logical Guess and Joshua EllisI used a radial bias function artificial neural network to do it.My neural network had layer sizes 10, 40, 160, 1 (I discarded the column month because it really wasn't much use)I managed to get 0.63182 and come about 2300thMy code is in C++ with openCV as my linear algebra library athttps://github.com/HenryJia/CVMLI'm still tweaking with it and playing with it to see if I can get it to do any better.",
    "The Washington DC data shows that (on workdays), people rent the most bikes both when the temperature is warm and it's around 9am in the morning or 6pm in the evening. This presumably falls around their morning and evening commutes.What insights can you find in this data?Share them through Kaggle Scripts Hi@benhamnerthe provided link was broken (Invalid)",
    "In typical time series data prediction, we have the training data set as say day 1 - day 200 and the test data or something we need to predict is into the future say from day 201 onward. In this case, the training data contains (day 1-20 of Month 1+ day 1-20 of Month 2 ….etc) and all the test data ( day 21 - 30 of Month 1 + 21 - 30 of Month 2 +…etc) is not necessarily in the future time of the train data.For this case, we were asked to predict using only information available prior to rental period. So, to predict day 21 - 30 of Month 1, the information available and can be used is just the info from day 1-20 of Month 1. So, if we follow the general procedure of using the entire train data set to select the model, determine the hypothesis function and then use this to fit and predict the test data, shouldn't this violate the given condition.Ideally, shouldn't we train using only the past data to predict the days 21 -30 of Month 1 and then retrain using available data for day 1-20 of Month 2 to predict 21 -30 of Month 2 and so on.",
    "Hi Fellow Kagglers,I wrote this tutorial using Scikit-learn's Random Forest regressor. I would like you to go through this and suggest some improvements.Bike-Sharing-DemandThanks,Shubham Thanks for the tutorial,with modifying rf parameters, i was able to obtain 0.40643- I tried binning: the deviation went up (ditched binning)-I tried log10 transformation of the predictor, the deviation went up (ditched transformation)- increased and interation and max split, the deviation went up again.Could anyone make a suggestions on what I could try next to improve 0.40643 score using the rf algorithm?also shubam's tutorial and sklearn document doesn't show any usage of training and testing dataset so for me, submission is the only way of knowing whether my work is improving or not,could someone please refer me to a way to test my model prior to submission?thank you in advanceEDIT: I removed additional variables which I derived from the original data. and applied transformation, my pub score is at 0.38347still looking for a way to improve it Hi,I found this advice on this StackExchange post.http://stats.stackexchange.com/questions/95212/improve-classification-with-many-categorical-variablesRandom forest can deal with categorical variable simply converted to a single numerical variable for each category. The invalid link provided or it might be expired. Could you please check@shubhamtomar Hi, I saw your rank is 4th in bike sharing demand and it's really cool. Could you please share the method you use and other tips?Jin L wroteThanks for the tutorial,with modifying rf parameters, i was able to obtain 0.40643- I tried binning: the deviation went up (ditched binning)-I tried log10 transformation of the predictor, the deviation went up (ditched transformation)- increased and interation and max split, the deviation went up again.Could anyone make a suggestions on what I could try next to improve 0.40643 score using the rf algorithm?also shubam's tutorial and sklearn document doesn't show any usage of training and testing dataset so for me, submission is the only way of knowing whether my work is improving or not,could someone please refer me to a way to test my model prior to submission?thank you in advanceEDIT: I removed additional variables which I derived from the original data. and applied transformation, my pub score is at 0.38347still looking for a way to improve it Good job with the tutorial. Very brief but also very understandable and a great result.Interesting to see how well Pythons SKlearn performs \"out of the box\" as compared to R's RandomForest. R produced a score of about 0.47 or so with the same combination of features as in your tutorial.@Jin: Easiest way to test before submission is if you build yourself a random subset of \"validation data\".Pick out a random 10% of entries from training set as \"validation set\".Train your random forest on the remaining 90% of the training set as usual (important here is not to include your validation set)Make predictions for the validation setCalculate the error of your predictions according to thekaggle formula(You can do this because the actual count values for the validation set are known to you)If you want to go the extra mile, checkthis. But this might be a touch too much for your case.Hope that helps. I think if you examine the training and test data you would see they are congruent, meaning that training and test data both have same 2 years and all 12 months. The only difference is that the training data contains the first 3 weeks of every month and missing the last week of each month, and the test data has the missing last week of every month not in the training data. If you put both together you would have a complete set of data but missing the count data in the test portion that is being predicted.Technically you could put the test data into the training data and then attempt to use Impute methods to arrive at guesses but that sort of defeats the purpose of this Machine Learning exercise. Hi, Shubham, thanks for sharing your work on the bike demand problem.Do I understand correctly that you construct one model to make predictions for all time? If so, that seems to allow future data to help predict the past, since the test set is made up of many segments interspersed with the training data.It seems necessary to build multiple models, one for predicting each segment of the test set. Each model can take into account the training data which has timestamps before the first timestamp in the test set segment. I wonder if you have tried anything like that? If so, how did it turn out?best,Robert Hi,I just found out that you follow a very similar path as me. I read your code and figured, that you used season and weather as numerical variables (correct me if I'm wrong) - not categorical.Did you try both or is there any other reason you did it that way?greetingschris",
    "Is bikesharingdemand.csv file still available? sure.you can usetrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")",
    "I think it all true about the demands and tendencies, but somehow when you are in a competing circumstance, what u can do is trying so hard to chase the high values users, in another word, put down bikes as many as possible for people who go to work at weekdays. Of course, base on the bikes you have, no matter how inferior it is.By the way, due to ofo's process of development, I think it is a good way to make a decision where to reassign more human resources by checking out all these data plus income and costs. This is so true. Supply and demand are important but there are other factors that matter in the equation.",
    "Based on the discussion in the \"Rules Topic\" in this forum, apparently a lot of people trained the model using the entire training set violating the assumption that you cannot use future to predict the past. Has any one tried building the model without violating the assumption ? If yes, what do the performance no's look like on the public test set ? Hello ! What method do you use in order to do that ? My best scored .45290 on the public set with the past data constraint. Theres actually not much clarity on this rule. Lets see how it unfolds. My best so far is .48308. I used R, trained a GBM for log(registered), and for casual users used mean(casual where test.month=train.month, test.weekday=train.weekday, test.hour=train.hour, test.date>train.date) since it gave better results than any model I've managed to run on it. I just got to 0.47645 using days 1-19 to predict 20- but unsure of what to do next. I am curious to see what the organizers will say about enforcing the rules once it's over. My best score is 0.49549 with just using past data, the beginning of the month to predict the end of the month. My best so far is 0.531 using a lookup table which averages log(count + 1) by month, hour of day, and workingday flag.It's really too bad that we can't tell whether a model uses future data ... I'm also curious to know what are the best scores which use only past data. Maybe the scoring could have been organized differently, I don't really know what's possible in that respect. My best score without violating the assumption so far is 0.81557 0.48932.I'd appreciate it if others would share their bests in this thread.Thanks!",
    "Initially, I chose DT as my benchmark model. But it turns out to be superior to both Random Forest and XGBoost. Interesting!DT: 0.655246 (0.121586)Random Forest : 1.164813 (0.133489).XGBoost: 0.776263 (0.067193).",
    "I'm trying to submit the below data to system and I get an error. Can you please help understnad the problem that we have with my dataset. Thanks.ERROR: Unable to find 6493 required key values in the 'datetime' column I had the same issue, it seems loading it into Excel auto-corrects the datetime column into a different format. I got around it by opening a fresh Excel worksheet and manually importing the csv file (Data->From Text/CSV) and then manually adjusting the data type for datetime to text. Hope that helps.",
    "I was just playing with ggplot and these are two graphs of Demand broken down by day and Hour.I haven't submitted any prediction yet, but analyzing these graphs it seems usefull split datetime into hour and weekday.What de you think? Hi Spearfisher,First, you need to arrange the data similarly to pivot table en excel, using hour, weekday and mean(train$count). I used reshape package for this, maybe there is a better way to do it, but this package works good for me.Then you can use ggplot2 package for the hea tmap and line chart.library(reshape)train$hour = as.numeric(train$datetime$hour)#pivot tableWeekHour=aggregate(count ~ + hour+ weekday, data =train, FUN=mean)#ggplot2library(ggplot2)#Line chartggplot(WeekHour, aes(x=hour, y=count)) + geom_line(aes(group=weekday, color=weekday),size=2,alpha=0.5)#Heat mapggplot(WeekHour, aes(x=hour, y=weekday)) + geom_tile(aes(fill = count))+ scale_fill_gradient(low=\"white\", high=\"red\") Hi, Can you share the syntax to create bar graph in R with 2 continuous and 1 categorical variableWould be great if you could share it at Email id: \"amanpreet.singh2702@gmail.com\" Hi Yogesh,This page should help you: http://docs.ggplot2.org/current/geom_smooth.html @preyas.The X axis shows time in hours, from 0 to 23 hours and the y-axis shows the mean of count variable.Each line represent the day of week (monday, tuesday ...).Let me know if you see something wrong with that graph. Hello Guys!Hope working hard to crack the problem :) .Can some please tell how can I label geom_smooth() lines in ggplot2.Thanks Hey espanarey,Is your legend on Hour-weekend.png correct? Thanks for sharing the code@espanarey, this is amazingly helpful Thanks for sharing the code.  Plotting \"casual\" and \"registered\" instead of \"count\" results in different hourly patterns for the two groups.  Casual users are more likely to use bikes in the afternoon, both on weekdays and the weekend (usage is greater on the weekend).  \"registered\" users are more likely use the bikes during the morning and evening work commutes during the weekdays, and in the afternoon during the weekend. Hi I am new to R and I'd like to get quickly used to it. Do you mind sharing the code you used to create the heat map and the line chart? Is there a link to a github repo?Thanks and regards Thank you!I tried to include \"weekdays\" as an additional variable. It actually performed worse than the one without it. :( Hi, R has a function called \"weekdays\", but first you have to change the format of datetime variable as date. These are the script.train$datetime=strptime(as.character(train$datetime), format=\"%Y-%m-%d %H:%M:%S\")train$weekday = as.factor(weekdays(train$datetime) Would you please share with me how you get the weekdays from the dataset? I tried in Excel but cannot get it. Thank you.",
    "I've lived in DC for the past two years and even though winters are pretty mild, it can still get pretty cold, definitely below freezing. So when I saw that the minimum values fortempandatempwere just above freezing (0.82 and 0.76 C, respectively), something didn't look right.I checked out Wolfram Alpha and they claim that the low for both 2011 and 2012 was 17 F, -8.3 C, in the month of January. Has anyone else noticed this? I feel like the difference in temps is pretty significant. Do we know how Bike Share comes up with the temps?...average over the area of DC?..from a single station in the District?Let me know what you guys think! From UCI ML where the original dataset is hosted :https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Datasettemp: Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)",
    "I have trained a gbm model to predict the bike sharing demand. I take the data before 13th day each month as the train set , and the data after 13th day each moth as the validation set. On the validation set , the RMSLE is 0.30, but the RMSLE on the test set is sadly 0.45. I con't figure out why.Any suggestions? Thanks in advance!! I also have a consistent gap between Valid and Test set losses, even across models and days inside valid set.I guess the explanation would be that the distribution inside the Test is just different and difficult to replicate in the Valid set.Train/Valid never have quarter-end, Christmas periods, thanksgiving day, end of year, etc.The good news is that once you know the gap, you can adjust your aimed target loss (RMSLE).One thing I did to investigate possible sources of big loss is to plot the loss with Pivot Tables. My error was big when count was very small or null (due to the nature of RMSLE).",
    "When i upload my results as per the format that has been provided, I get this result:\"Unable to find 6493 required key values in the 'datetime' columnUnable to find the required key value '2011-01-20 00:00:00' in the 'datetime' column\"Can some one please tell me the reason behind this. The problem is with the csv file. Here is what is did to solve the problem:Open Excel go to the DateTime column2.Select all rows from the datetime column3.Right-click and go to format cellsGo to custom and use this format yyyy-mm-dd hh:mm:ssThat should about solve the problem. You have 2011-01-20 00:00It needs to be 2011-01-20 00:00:00You are leaving off the last ':00' I tried including both the columns into (seperated by a comma) to see if that makes a difference. Nothing happened.I also looked at text editor and over there it comes out fine but i really am not getting whats the issue. The first line of your file is:2011-01-20 00:00,30.37287304The first line of the sample submission is:2011-01-20 00:00:00,0Always look at your submission file (in a text editor) when you have bugs! Hi William,I had used the submission Sample file that was provided to update the predictions. I guess, that should have worked but thats not working either.Kindly help in this regard.ThanksShantanu The format of your datetime column is either not matching the required format or does not have all 6493 values. Investigate why '2011-01-20 00:00:00' is not in your submission.",
    "Did anyone used AWS ML (LR -> SGD) to predict bike sharing demand? It is bit of circus to use AWS ML Linear Regression model to predict the bike sharing demand. The first score was 1.21. I was able to improve it to 1.001 by turning off the regularization and using different interpretation for negative predictions (still trying to figure out why the model is throwing negative predictions). Just wondering if anyone went through the same gyrations.",
    "I have been getting a score of 0.56 without including the year dummy variable. After including the year variable in my model, the score tanks to 0.76 It is pretty obvious that the year is a significant variable in this model but not able to figure out where am I going wrong. Would really appreciate if someone could explain this discrepancy or any other suggestions to improve this model as this is my first attempt at solving Kaggle problems.Following is the link to my kernel:https://www.kaggle.com/miteshyadav/predicting-bicycle-usage-count-using-rfregressor",
    "Hi! I really need solution in R language. I saw different solutions on GitHub. Most of them have higher then 0.40 score or written in C/C++. One has great result - 0.43, but I cannot understand how this result was obtained. Maybe my experience is not very great. Can you help me? I need sequential solution in R with score less then 0.40. Thank you very much. Hi.This could help https://www.youtube.com/watch?v=yLYLgJHPOUI this video is not available…",
    "Hi guys,I'm getting an error while submitting my file for the bike sharing competition. Can anyone help me out with this issue.Error: Unable to find required key value '2011-01-20 00:00:00 in the datetime columnRegards,Sunil",
    "Hi,I would like to share my solution. Implementation is done on Apache Spark 2.1 (pyspark).https://github.com/artursuchocki/kaggle_capital_bikesharingCheers.",
    "I have got the prediction vector with the right number of elements (6493), and now  I am unable to make a correct CSV. The grader keeps saying: \"Unable to find 6493 required values in the 'datetime' column.\"I am completely new to python. Please help me to make a proper file.write() code assuming I have a dataset.shape 6493 to 2 containing datetimes in the first column and counts in the second. Can you paste the first few lines of your submission file here? datetime,count2011-01-20 00,14.622011-01-20 01,5.722011-01-20 02,5.862011-01-20 03,3.562011-01-20 04,3.122011-01-20 05,16.26 Also:datetime,count20.01.2011 00:00:00,14.6220.01.2011 01:00:00,5.7220.01.2011 02:00:00,5.8620.01.2011 03:00:00,3.5620.01.2011 04:00:00,3.1220.01.2011 05:00:00,16.2620.01.2011 06:00:00,61.08 You should format your  date with dashes (2011-01-20) and your time with colon (00:00:00) then put them together (concatenate) with a space between like this: \"2011-01-20 00:00:00\" then use a comma as a separator to the count. A correct row should look like this: \"2011-01-20 00:00:00,5\"Your first submission file has the correct date format and your second submission file has the correct time format. So you are on the right path.. :) But you need to have correct format on both date and time.So correct submission should will look like this:datetime,count2011-01-20 00:00:00, 12011-01-20 01:00:00, 4…",
    "We’ve just released Scripts, a new feature that’s only enabled here and on the digit recognizer competition (for now).Check out some cool examples of scripts we've written:Bike Rentals By Time And TemperatureRandom Forest BenchmarkBike Rentals By TimeThis feature enables you to run R code directly on the bike sharing dataset without needing to download the data or even leave your browser. We’ve preloaded the environment with ourfavorite R packages. You can get started bycreating your own script.The goals for this are twofold. We want to make it frictionless to get started on a dataset, and we want to make it frictionless for you to share cool insights you’ve found.Without Scripts, to get started on the dataset you need to download the data, download the sample code, and setup your local computational environment. This can be painful and time-consuming. With Scripts, all of this is preloaded in your browser - all you need to do is press “Run” to generate a new set of results. You can tweak and edit it from there.We don’t expect you to stay in this environment for very long: your favorite analytics environment (whether it’s RStudio or iPython Notebook) forms a far more refined interactive experience. Once you’ve made a couple passes exploring the data and tweaking the examples, it makes sense to work locally to iterate faster.The second way we find Scripts compelling is to share cool models and insights on the data. Right now, users are sharing code and insights by posting on the forums and attaching code/image files, or linking to externally hosted blogs. This is great, but it’s still tough to recreate and build on these results locally. Scripts provide a far better way to do this for any code that it can currently run: you can paste the code into Scripts, press run to generate the results, and then share the combined code and output with the world.To give you a sense for some of what's coming next, we're currently working on Python support as well as polishing many aspects of the design / user experience for this feature.Please let us know about any issues you encounter using Scripts, as well as ideas you have to make it better. We’d love to have your input as we build on and refine this feature. I stumbled upon this discussion. The scripts provided in the text are not working anymore. I know it's been 4 years ago, but nevertheless it should be mentioned. I need helping to write python, R hadoop and map-reduce Good concept. But , note that this actually adds more work for modeler. I will need to convert my r script in native environment to Kaggle R script. And, I am not even talking about python. It is probably easier for me to snip the screenshot and paste on forum.But , all and all it is good innovation. At the least the newbees will get to try around some realscripts I saw it, Ben! Looks great!Just a a miscellaneous... I think it could be a nice blend if rpy2 is included... ec-ccs wroteWaiting also for python. Please version 3.x too.Coming soon, hopefully before the end of the week :)From my perspective, the only Python version that exists is 3.4.3. Thanks Ben!Waiting also for python. Please version 3.x too. Thanks a lot for the sharing.",
    "Hi everyone!I'm not a professionnal and neither a computer scientist, so I didn't use R code for this problem but Visual Basic, and I get good results (0.49) I think.Someone else like me? Using VBA? I am using",
    "HI I have two data set one for bike share usage and the date format is  like this 04/28/2017 12:00 P.M. and another one for weather with 04/28/2017 format. I want to find any possible correlation between ( length, duration, and trip start and destination) which are presented in bike share usage file and weather  elements.I do not want to emerge the data using excel because the dataset record is more than 20 million records and it is not possible. I want to see what is the best approach to do it with highest efficiency.",
    "Hi. I just finished compiling a blog on my entire approach and the code using random forest in R.link:http://bit.ly/BICYCLERHope this helps and would love your suggestions to improve this.Cheers. Hey Himanshu,I like your post!  A few suggestions: For date manipulations I'd look into the lubridate package, it should make your life easier. You could then do all the date transformations in one go like this:bike <- bike %>% \n       mutate(hour=hour(datetime),day=day(datetime),month=month(datetime),year=year(datetime),\n             wday = wday(datetime))content_copyAlsocutmight come in handy if you want to split a numeric variable into factors.And I think you could just use the predictions from your trees as features instead of manually encoding the trees. So just have a new feature beas.factor(predict(tree1, full)).Good luck!Alex awesome post by Himanshu. Also Alex, thanks for your advice. You too are awesome :)learning alot!Germ",
    "Please, could the administrators fix the submission script? I've tried everything I get an Error saying the first date cannot be found. Hi,I removed the double quotes (\") around the dates (may not may a difference, but best to make it exactly like sampleSubmission).The real problem here is the dateformat. You submit:1/20/2011 0:00Kaggle wants:2011-01-20 00:00:00Fix that and you should be good to go.Good luck! https://www.kaggle.com/c/bike-sharing-demand/details/evaluation Anjana Agrawal wroteHello,Need support from the system administrator as I am getting submission errors. I have used the same program and have made 10 submissions without any error. No change has been made in the program , In fact I have tried using the sample submission format as well.. That too is giving an error. Pl. helpRegardsAnjanaCheck the format of your dates in your submission file (especially if you are using Excel). Thanks, the formatting for the date workedRegards,Sunil No, I just downloaded it again and that's the format it came in to me. So maybe it's how the download occurred? I'm not sure, but I didn't do anything to the raw file that I downloaded.Anyway, thanks for the help. It sounds like this is the problem...EDIT: Apparently when I open the .csv in Excel it reformats the field to show it in the wrong format that I was seeing, and then that format gets retained.If I view the file using Notepad it keeps the initial format. So as long as I don't open the .csv in Excel I'm OK.That was annoying but it looks like I got through it...my crappy model is on the leaderboard.Thanks again... The sampleSubmission that I just downloaded had the correctly formatted dates. I think somewhere when outputting dates you yourself change the format. OK I will try that...is it odd though that the sampleSubmission.csv that was provided doesn't format the dates in the way that the data is supposed to be submitted???? It's uploaded in this post. I'm assuming that's OK to do; believe me this one is not going to be anywhere near winning the competition. I'm just trying to get my feet wet...Thanks again for any assistance. I think floats are ok to use. Perhaps look if you got all ID's correctly?It is hard to debug remotely. If it fails, feel free to upload your submission with a post here, and I (or someone else) will have a look. I did that and didn't see anything amiss. I noticed the commas and tried a version where the csv had only one column, and had a comma inserted between the two fields. That didn't work either.I'll try poking around a bit; the submission format seems to be the easiest thing in the world so I don't know what I could be doing wrong, but like I said this is my first Kaggle submission so maybe the way I'm creating it is off. The number of records matches (6493).The only thing that could be off is that my predictions aren't integers. Do they need to be? I didn't see that specified.Thanks... @joellahrmanYou can find the sampleSubmission.csv on the data page. Then you can compare your submission to see if anything is off.The first line of the sampleSubmission is:datetime,count2011-01-20 00:00:00,0Notice the comma's.Your submission should have predictions for 6.493 test samples. Hi all, I'm trying to make my first ever submission and I'm having pretty similar issues.I'm working in R. I take the sampleSubmission.csv file, delete the count variable, then append my count column, and save as a new .csv. This is all done in R.But I continue to get the message seen in the attached screenshot (sorry I accidentally posted it twice).Anybody have any idea what I can do? The first line of my csv is:datetime count1/20/2011 0:00 18.76461313 Thanks... I am able to resolve the problem... Thanks for your revert.. What should be the format... Hello,Need support from the system administrator as I am getting submission errors. I have used the same program and have made 10 submissions without any error. No change has been made in the program , In fact I have tried using the sample submission format as well.. That too is giving an error. Pl. helpRegardsAnjana Thanks Ultron... I dont know Python, I use SAS for the analysis. Looks like I can change and save the date in the format you said in xls format, but while saving the document as CSV, the previous date format returns  and the solution needs to uploaded in csv format only.Anyways, thanks much for your time.Thanks,Varun Hi Varun,The first two lines of my file are:datetime,count2011-01-20 00:00:00,14.0I advise using a python script to make the entire filefor x in range(0,len(data_from_test[0::,0])):dates.append(str(data_from_test[x,0]))prediction_file = open(\"C:/.../prediction_1.csv\", \"wb\")prediction_file_object = csv.writer(prediction_file)ans = zip(dates,predictions)prediction_file_object.writerows(ans)Where \"data_from_test\" is the object you load the test.csv file into Hi Ultron, which format did you keep the date in? I see sample submission shows date format as DD-MM-YYY.Hi Admin,I have attached the document that has screen shots of date formats in sample submission and my submission. I used SAS to arrive at the results.Please helpThanks, Varun I have been able to resolve my issue thanks. I built the submission file in python without passing through the Excel route.Best! I had the same issue but be able to overcome it by reformmating the datetime column into the right format with excel.the right format is \" yyyy-mm-dd hh:mm:ss\"Try this:) Check that your software is not modifying the date format (Excel will do this). I have the same problem with my submission. How can we do?",
    "How amazing! After I remove the \"month\" feature. I get a much better result.The model used in my code is GradientBoostingRegressor.At first, the feature of my data is \"year, day, month, hour, season, holiday, wkday, weather, temp, humidity, windspd\". The score I get is 0.41963.But after I remove the \"month\" feature. The score I get is 0.37188.I don't know why this happens. But it really works. Does anyone know why?",
    "Can someone help me split datetime variable into two separate variables- \"date\" and \"time\". Also, it seems the datetime variable is factor and when I tried to convert it using as.Date(strptime(train$datetime, \"%y-%d-%m %H:%M:%S\"), I keep getting NAs. I tried using the grep function and lets just say I failed pretty badly. Any help is appreciated.",
    "main points are:1. predict casual and registered count separately.2. instead of predicting count, but predict ln(count+1), because evaluation is RMSLE3. parameters of random forest are n = 1000, min_samples_split = 11(if use RF only, score is 0.38149)4. parameters of GBRT are n = 100, max_depth = 6(if use GBRT only, score is 0.37200)5. by averaging RF and GBRT, we get better result(0.37108)Another thing I want to share is, it's very important to choose validation set(for choosing parameters, for seeing how well our model works). At first I use scikit-learn built-in KFold implementation to do cross validation. But then I find maybe it's much better to choose time contiguous samples as cross validation set, because the pattern is much close to the pattern in test set. In my code, I choose validation set by day. For example, train samples which day is in [1~9, 12~19], validate sample which day is is 10,11.I don't do much feature engineering. I think by carefully transforming and combining some features, maybe I get better result.Here is thecodefor your reference. Sergey, I think you are right. In essence, it's because there are some extremely high values in data set.But in kaggle competition, when you train a model, you must want to high score on test set. Since they evaluate your model using RMSLE, so you better to train your model using RMSLE too.In another words, even they are some extremely high values in data set, but they change evaluation method to RMSE, so I think we'd better to train our model using RMSE. Because I think that might give us higher probability to get high score.Correct me if I'm wrong:) dirlt, are you sure it is due to evaluation method? not because you reduce the influence of extremely high values which are very rare?you use min sample split 11 - in case you have one extremely high value in this busket - your all predictions would be highly overestimated. with log transformation the level of overestimation is going to be much lower.I used log transformation because of highly right skewed data distribution. Could you please, clarify, why algorithms work better if one takes log(1 + x) from the count value? Simply becauseevaluationof this problem is the difference between log(predict + 1) and log(target +1). The code is not avaliable, can you submit a new one? I think you get better score when your error on a test set is minimum whatever the evaluation method. In case some quadratic error would be used you won`t get better score just squaring registered and casual. I believe we get worse score with quadratic transformation and better one with log.",
    "I am trying to use Linear Regression. As bike rentals are not linear w.r.t hour, simple linear regression does not perform very well (score ~1.5).So I broke down each hour as a feature (leading to 24 features, namely from 0th hour to 23rd hour) and a linear regression on these 24 features bumped the score to about ~0.85.At this point, adding any new features seem to only decrease the score. Any thoughts onIs breaking each hour into 24 features (or every 2 hours into 12) a good idea? Why does adding new features not help anymore?How about performing two stage linear regression? One linear regression on hour (after breaking it down to 24 features, since simple linear regression is not suitable for non linear feature) and other on linear feature like temperature, wind etc, and a second stage linear regression on both these regressions?Any other suggestions on improving linear regression?Regards,A novice Kaggle user I am not sure anybody is working on this anymore but here are some of my comments and results with linear regression.Based on the principle of creating one feature by hour, I extended the concept to create a new feature for each combination of month/year/working day and hour.  I end up with a RMSLE of 0.3689 on the training set and 0.535 on the test.One good thing is that it is not using future data to build prediction. Only the first 20 days of the month are used to assess the linear regression parameters.I think improvements can be done by including weather data but I could not come up with any way to do it. If anyone has some ideas, I would love to hear them. Generally it's a good idea, but you would have to make it 23 features (as the very last hour can be perfectly explained by the first 23) to avoid multicollinearity in your regressors.BestJonas I think this is due to overfitting. Just feeling. What I did was using a k-mean to cluster hours together. It improved results a lot for linear regression, but not much for random trees. At least, it stopped decreasing results. Just did the same and had the same output.  Interesting questions I can't solve :p",
    "Model :sklearn.linear_model.SGDRegressorsklearn.linear_model.Ridgeboth model are predicting negative outputX = Index([u'season', u'holiday', u'workingday', u'weather', u'temp', u'atemp',u'humidity', u'windspeed', u'hour', u'weekday', u'month'],dtype='object')y = 'registered'weekday ( 0 - 6 for sun - sat respectively )Any idea what might be going wrong, or how to handle negative values. I figured it out: it was in fact an error in my RMSLE computation, since entries where negative values were predicted were converted to NaN, and then left out of the final average. So the first values of e.g. 0.83 were wrong. Beware of RMSLE with linear regression that can produce negative values! I tried this, replacing predicted values < 0 with 0, using basic linear regression, and curiously enough, my RMSLE was much worse, from 0.83 to 1.11. How could a negative value ever be closer to the real score for that hour than 0? In fact the absolute mean error was reduced when I replaced values, from 74.7 to 69.4. Is this a peculiarity of root mean squared logarithmic error? Thanks Robert that was really helpful, will try out your suggested approaches and see how it works. Well, models which are linear in their inputs are unbounded -- for large enough inputs in one direction or another, the output will be as large (positive or negative) as you want. So if the output needs to be bounded, you have to use some kind of nonlinear model.For the bike sharing problem, I see at least a couple of reasonable ways to do it. The simplest is just to take the output of the linear model and output zero if that's negative. The next simplest is to take the logarithm of the bike demand (taking a large negative number if demand = zero). Then to report predictions, take exp(y) where y = output of linear model. There are doubtless other things to try.",
    "Hi all.I want to share my snapshot of IPython notebook, which you can use to write your own solution for this competition. I uploaded training and test sets. I also wrote simple solution with RandomForest. You can use it as starting point for your own solutions.This snapshot will be especially useful for newcomers, because all libraries are already installed for you. Also you can add more RAM and CPU to your instance \"on fly\", which I found very useful.Here is link: https://www.terminal.com/tiny/ktoT6KfqDi . I'm going to create similar snapshots for other Kaggle competitions, so I would be glad to hear your feedback. Hey Noman.If you are completely new to ML and Kaggle I would recommend you to start from very basics and theory. There are a number of great online courses about statistics and ML. I guess this one is the most famous: https://www.coursera.org/course/ml . You can find more courses on this pages:*  https://www.coursera.org/courses* https://www.udacity.com/courses#!/all* https://www.edx.org/course-searchIf you don't need theory, I can recommend looking at this page https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks . It is large collection of different IPython Notebooks, you can \"learn-by-doing\" using those examples. I also made interactive snapshots from some of those notebooks. you can find them at this page https://www.terminal.com/explore#query=username%3Aamezhenin&sortby=popularity . Take a look at this snapshot first https://www.terminal.com/tiny/8ePy9HmEyk , if you decide to have a brief introduction.Talking about this solution: it is very simple, but I don't think random forest is really suitable for this problem. You should try other algorithms first and see how they perform. hey , Thank you for your reply ! so ,how about telling us your new link that you create snapshots for other Kaggle competitions? Thank you so much ! But the linkhttps://www.terminal.com/tiny/ktoT6KfqDiseems does not work! can you please tell us a new link　？ Hey Joeseph. Looks like they have largely reworked their site. I can't find anything there :( The link does not work anymore I got and thanks for helping me in such a good way ;)If there shall be confusion i will communicate with you :) Hi,Artem can you please guide me,how i can improve accuracy/score compared to 1.18999, i have tried  your method/solution but i want to improve the scores.Actually i am new to kaggle and machine learning,please guide me :)Regards.",
    "HeyLooking to form a team of 2-5 people in bay area so that we can meet 1-2 times a week and discuss Kaggle projects.Please e-mail me if interested.ThanksGaurav Hi Gaurav ,I am interested in forming a team as well . Let me know if you are still interested !Thanks.",
    "Hi, are you still \"Closed - Validating Final Results\"?ThxEugen",
    "How badly will Linear regression fare in this particular case as compared to Tree based regression? Linear regression can give scores < 0.4. The key is feature selection. Using linear regression for \"casual\" & RF for \"registered\" gave me a score of 0.434.  RF gave a variance explained of ~84% for registered, while linear regression gave much lesser. Hence I had to use RF for registered. If I play around features, there might be room to improve score here ( while staying between linear regression & RF). Josh Hanson wroteWhat about fixed effects regression models?  Using fixed effects, I achieved an r2 of about .84Hi Josh, do you have procedure to conduct the fixed effect regression? Which variables do you take to set up as the entity variable? Thanks Josh Hanson wroteWhat about fixed effects regression models?  Using fixed effects, I achieved an r2 of about .84Hi Josh, do you have procedure of conducting the fixed effects regression? I am not sure how to set up the entities variable. marian wroteLinear regression can give scores < 0.4. The key is feature selection.Hi Marian!Now rhe competition is over, could you explain your linera regression method that gives scores <0.4.At least, the featues you have selected, and are they different for casual and registered? Hello Marian,Can you guide us how to use Linear regression for getting scores < 0.4 Hi all,I have been working some linear regression for a different approach. At the time of this writing I haven't delivered a full description of the model though, but please have a first look of some methodological comparisons atthis thread. Hi Adrian JaskółkaCan you elaborate a bit more? I tried using linear regression but the R^2 is only about 30%.Also there is a lot of heteroskedasticity.Not sure which other model can be used here. I have tried some of them and they were bad (around 1 on my test set, compared to ~.4 of random forest).I should try them again with some different considerations:normalized valuesonly the most important features.",
    "I'm trying some algorithms, but dividing training 70/30 or 80/20 local and running the metric it gives me results that make no sense compared with the ones the site gives me.For example a basic Random Forest in my pc gives me 4.8, but when I upload it, it gives me 0.70.I'm splitting the dataset this way:trainOrig <- read.csv(\"train.csv\")set.seed(234)inTraining <- createDataPartition(trainOrig$count, p = .8, list = FALSE)train <- trainOrig[ inTraining,]test  <- trainOrig[-inTraining,]and evaluting it with:rmsle(test$count, saved)or made by myself:n <- nrow(test)interno <- 0for(i in 1:n) {interno <- interno +(log(test$count[i]+1)-log(saved[i]+1))^2}sqrt(interno/n)I tried to change the split and the seed, but it's always the same. Solved, I had a mistake in my code",
    "Hey folks,I'm interested in knowing what formula/methodology was used to calculate the variable \"atemp\".atemp is meant represent the \"feel like temperature in Celsius\". Thing is, there are multiple ways to calculate this. There's the \"heat index\", the \"humidex\", or you're even got the \"Accuweather RealFeel Temperature\". Sometimes it's just temperature and relative humidity, and sometimes it can include other factors such as wind speed or dewpoint.Is the formula used to calcuate atemp known (probably an Admin question)? I'm trying to not have to reverse engineer it, if possible. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)Source:https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset",
    "I tried to apply this analysis over the dataset with R, but I got no visual succeed.Does anyone got good results doing that?",
    "I not found in rules for this competition that final results will be calculated on different data set, does it mean that final results will be calculated on the same data set, which is using on current leaderboard? I dont' understand why in this case we should choose 2 decisions: \"If you do not choose any entries, your top 2 entries from the public leaderboard will be chosen by default.\", why just current place in Leaderboard can not be used? I'm very interested in it too. Correct. This dataset is public, which means people will cheat on it, which means there isn't much reason to have a holdout private set.",
    "Hi All,I have been facing the same issue as others. I can see in the thread that, it has been mentioned that,change the format of the datetime to yyyy-mm-dd hh:mm:ss.I did the same thing, but still getting the same error. How to resolve this problem?I attached the snapshot of the format Use a text editor to check the format. Excel can change the date format when you export it.",
    "I got an idea that turned out helpful. The coefficient of variance of the registered and output users is different. 0.97 for registered and 1.38 for casual.This makes sense as casual users (in the real world) have a larger range - anyone can be casual - than registered users.This implies that: For the same input data, casual count has a larger spread and hence, your algorithm can have a larger degree of overfitting on this column.I'm using an ensemble method and tweaked it to 'try harder' on the registered set. This gave me a reasonable improvement.Point is, if you're using the same algorithm on the two outputs, one should have different parameters.Let me know what works for you!",
    "HeyLooking to form a team of 2-5 people in bay area so that we can meet 1-2 times a week and discuss Kaggle projects.Please e-mail me if interested.ThanksGaurav",
    "Can someone provide me a summary of strengths and weaknesses of different kinds of prediction algorithms?Also, for each algorithm, which type of variable (numeric/categorical) can be used for predictors and response?I really got lost after diving into the material found on the internet.Thanks a lot!!!",
    "Hi Friends,I'm learning machine learning and looking for regression analysis related competition to practice. There should be multiple features and needed to be picked up carefully.Can someone suggest me few such problems?Regards,Sidharth https://inclass.kaggle.com/c/model-t4",
    "How to create new variables and assign them value  in “R” in a for loop.I triedfor (j in 0 : 2){foo[j] =j}But it is not creating  variables.I want output to be:foo0=0f001=1foo2=2Any help appreciated. You should use the functions assign() and paste().for(j in 0:2){assign( paste(\"foo\", j , sep = \"\"), j)}",
    "Am I crazy or are the 20th to 30th or 31st missing from the data set? I can work with this, but if the data is available somewhere I am not looking, please let me know. the 20th-30th/31st days missing are the days you are to predict. There is another dataset (test data) containing the variables for the missing days.",
    "Hi, has anyone tried to analyse the data by applying time seriers approaches, such ARIMA ? Is the output of ts analysis helpful to improve the model ? I haven't done much time series before, so the competition seemed interesting. I thought we were using time series methods, but after signing up saw the rule about using random forest.I do not know if we can compare both methods, but I will use the random forest method first. Afterwards, I can apply a time series forecasting method and see if I can predict count for the next 10 days every hour. I converted the bike dataset to a ts object using as.ts() and will do some visualizations like this tutorial says: http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html.If you plan on using time series methods, it would be great to bounce some ideas off each other.",
    "im medhi, im in master degree machine learning, i work with python a lot, im looking for a teammate  to work on this competition.email : zinounemed@hotmail.com I am Gideon. I has Masters in Business Informatics with focus on Business Analytics.I work with SPSS and R - averagely. I am willing to join your team. Let me know what you think. I sent you email already.email: gideonaina@yahoo.comThank you.",
    "Hi All,I have been using python scikit module for model evaluation. Its been quite a while since I practiced coding in R. I find grid_search scikit modules for parameter evaluation and model.feature_importances_ (where modei is classifier object) for feature engineering pretty useful for optimizing model parameters. Are there similar modules that I can use in R for parameter optimization? http://topepo.github.io/caret/index.htmlhttp://cran.r-project.org/web/packages/fscaret/index.html Great..pretty useful..Thanks Pardon that flagrant spelling mistake in topic heading..I meant \"Classifier Parameter Optimizations in R\".Thanks",
    "Hi guys,I am very new to data science and to this website.I just made my very first submission but there is an error like in the attachment.I dont know why because i made the submission similar to the sample, csv file, two columns.Please help me. thank u a lot!!! ah, solved it, the datetime column needs to be reformatted",
    "Hello all,I just recently entered this competition and am completely new to anything data science. I just tried to run the simple regression in R:PredCount <- lm(count~., data=train),where train is the training set. I merely did this to see what would happen (I don't expect it to provide fantastic results). However, the regression did not even finish in two or three minutes. Is that normal for the size of this data set, or did I do something incorrectly?I appreciate any guidance. I apologize for posting this if a similar topic has already been discussed. Thanks! Hi Nandaanandam,Yes, it's normal for a dataset of this size. Given that you're including all predictors in your model, it will take time to process. Of course, it depends on the processing power of your computer too.Gaurav Hi Nandaanandam,Yes, it's normal for a dataset of this size. Given that you're including all predictors in your model, it will take time to process. Of course, it depends on the processing power of your computer too.Gaurav",
    "Hey folks,My predictions are not rounded numbers but I was curious whether we need to round them before submission or not? I couldn't find anything in rules section or in the forums.Thanks,Faraz.",
    "Hi Kagglers!Some of you may know by now that, following a misunderstanding of the rules, there have been at least two formulations of this challenge. Let's clarify those formulations using scenarios:In thefirst and most popular onea client comes with a dataset where some data have disappeared after a blackout and the role of the analyst is trying to fill in the gaps. In this case, you have access to the full data set and can predict the missing values in any direction.In thesecond one(the correct one according to Kaggle team at \"Rules\" thread), the client comes with a dataset every 19th of the month at 23h and asks the analyst to predict the rest of the month. The analyst has access to any prior data but those previously non-observed days to predict the next 11 days. Missing data are kept unavailable \"due to confidentiality reasons\" (let's say...).You can easily see that thefirst problem is oneofmissing values imputation. Solutions to this problem so far have proved that it is a lot of room for improvements and that could be very interesting to solve.The second onefallsmore into the predictive analytics traditionand it suits more my interests, despite of having some \"anomalies\" in its Kaggle's formulation.Unfortunately working on the second problem won't be facilitated by the current situation:The leaderboard is unfortunately polluted with the results of both approaches, and the comparison is likely difficult if not impossible.In reality you normally expect to see the actual values of the prior non-observed dates afterwards to correct your model and sometimes a hour-to-hour approach could be implemented.Kaggle's global measuring doesn't help much to trace success when working on the second problem which is more a month-to-month analysis. Kaggle team at least provides the error metric used for this project, something the analyst should take in consideration when measuring success per month.As I have mentioned in other threads, I am currently interested in the second scenario. I would like again to call anyone who is interested in solving this problem? Would you post at least your results and approach so we can share benchmarks?/// NOTE: an extended approach, eg. using the solutions of the first problem to help solving the second problem could be also interesting. ///===================BONUS LINK // Later // Here I show the RMSLE figures and discuss the combi results.To help viewing the results I am attaching a figure with what I got as RMSLE per month per model per dependent variable (casual, registered and the average sum of both). From the current results we could suggest:- Models/Methods predicted casuals with less accuracy than they did with registered.- Some insights about the variability along the time horizon comes out. Some models didn't maintain control over variability along the period but could have a good result overall or viceversa. These preliminary results suggest, for example, that models like lm long could be less variable but the model used seems biased in general.- I applied the proposed RMSLE (average of ....) toconstruct a combi tool consisting in selecting the best performing model between lm long/short and rf long/short every month and using the selected model to predict the values for that month. However the data in the graphs suggests that the overall result should not be too different than using rf long alone (i.e. the combi selector selects almost always the rf long model every month). That is, none of the models really overperformed the monthly performance of rf long at any month, being in the best cases only similar in performance.(PD. maybe tomorrow I will know as I don't have more submissions left for today...).EDIT: I eventually loaded the combi file and effectively the overall results were the same as my best result (rf long), only slightly better at 0.0003....==========================OBS!!!!EDIT 2: I did a combi for the \"prior data only\" problem usingrf short and lm short(those which required less train data) andlog10(DV)instead of log(DV). This combi was 0.496 at leaderboard, no much better than rf short alone as I did for previous analysis, rf identified as \"rf clean model\" (find those resultshere). I added a figure of year performance. It appears that the variance along the time horizon is better, so perhaps it is a good control of the peaks, but I don't have enough data to say this way of combining models is usually better option, at least for this problem. @kmile: how are you doing, kmile? Something to share?Hi rest of kagglers,Here I just want to summarise an exploration of different approaches to predict the values of count with only the data on the LEFT. I was using simple models (LinReg and RanForest) and a mix of them using different time spans. I will eventually post the material and code (python3), for the moment scikit but possibly rpy2 as well. OBS: be aware that given the simplicity of the methods and in order to reduce the run time of this exploratory evaluation I didn't implement any cross-validation.The scenario I would like to consider is the following: suppose you are asked by a client to take care of this data for the next 2 years as mentioned. What do you do? By now, we are ok because we can see all the data and try different tools. But if you are seeing the data for the first time the decision of what tool would be better is difficult. Either you have substantial experience with a similar problem or you have to try different tools along the way. Additionally, depending on his/her requirements, a client might not be happy if you wait for 2 years to find a good solution: they could ask PRECISION EVERY MONTH.The proposed scenario suggests that we should check monthly performance, selecting models and approaches that offer the lowest variability (not necessarily the best overall precision).I also thought about controlling variability and improving precision by designing a mixed tool approach. If the data is complex, one possible solution to solve a variability problem could be not to stick to a single model (although this is likely the usual cost effective solution). I explored the combination of 2 models of Linear Regression and other 2 for Random Forest each with different sizes of training sets, first individually and then combined.Observations:In summary:------- Theamount of prior data(and therefore the training set size) to use could affect the model's predictive power differently. The sensitivity to noise and or the number of points required to predict are important here. The other aspect is the run time performance: in general the more data you use the worst the run time performance.------Designing a good TESTcould be tricky. I decided that that one I used previously (the last five days of available data) was good for this project.------Metric is relevant. In particular, the RMSLE calculations I have been getting are not helping. I calculated the RMSLE for my final test set's 'counts' (the rmsle calculated over all my predicted test counts, NOT THE TARGET SET to be predicted!) to evaluate overall precision but when I compared my overall RMSLE results for any separate model against RMSLE at leaderboard, the discrepancy was ENORMOUS. It is likely that my RMSLE for count was incorrect, and even all my RMSLEs were incorrect. Still I found an out-of-the-hat solution that even if not of the same accuracy as that for leaderboard (likely mine is biased), being less optimistic, it gets closer to the same trend: I calculated (rmsle_casual + rmsle_registered | method).Results:To list my results:(method, training data length) -> (final rmsle 'count') > (aver final rmsle cas/reg) -> (rmsle at lboard)rf, short ------------> .39 ---------> .55 --------------> .51rf, long -------------> .38 ---------> .53 --------------> .44lm, short ------------> .41 ---------> .57 --------------> .52lm, long -------------> .40 ---------> .60 --------------> .62Not having a better metric for now, I will use that to formulate a combi.EDIT:I haven't tested but I think the answer could be athere. The formula would have the following form:total_rmsle = √(∑j(∑i(Yij−Ỹij)^2)/Nj)/k)Y = log(estimated value+1); Ỹ = log(actual value + 1); Nj= number of observations; k = number of dependent variables to be estimated (in this case 2: casual and registered)What I think it is happening is that by splitting the analysis into two separated random variables, we should consider that each will contribute differently to the error analysis.Taking individual performances of the models, my best model so far with this simple approach is RanForest using progressively all the data on the left side. It beats a previous .495 I got by using fixed training set sizes (seehere). This could prove important if for example a business setup should be under consideration. The problem for my current best approach? It is slow and get slower as the training set increases in size. If the project would be for an implementation within a company environment, for example, it wouldn't be cost effective. The reason I'm interested in this challenge is exactly what you mention: creating a model that predicts based on prior data, optionally updating the model when new actuals become available.We haven't started yet, but I'll definitely update my results as we have something to show.",
    "When i try to make a submission using the submission format given, it throws an error and I dont know what could be the reason.Can anyone please help me out here. I have attached the error i am getting.ThanksShantanu",
    "Hi kagglers!Observations of the things that seem to be working for me so far (and some that not...).Be aware that I will be mentioning public code made available by other kagglers.To date, I have been concentrated on random forest and rf variations as method, but I will be experimenting with other ones in the near future. Additionally I will also talk about python scikit-learn and R.1) A first overview of my observations about using either R or python are mentioned at another thread(different languages..., different results?). There are other works that suggest some differences in the results when using those tools with random forest, in particularNew Bayesians(scikit-learn) andbruschkov(R randomForest package). I confirmed those differences, even when using New Bayesians model in R's randomForest: there was an improvement over bruschkov model but not substantial.Although this comparison is not \"serious enough\", this situation still posts a question mark:to what extend we can replicate similar results using different languages or packages that claim doing the same thing? The problem is not new and has been commented before by more serious analyses. Normally the identical performance is given for granted, but is it true? But if there are differences, then the problem is serious, specially if you have to select a tool and learn a language. A better understanding of the algorithms and corrections used at any package and likely the maintenance of the package could play an important role.2) Looking at the results I got using New Bayesians model itlooks like for random forest the no categorization (specially for time-related variables) is actually preferred. This could make sense:time is not a categorical variable. I personally thought that a categorization could have helped the model: if I understand correctly, random forest and any other method based on decision trees would convert any numeric variable into a range, which is an additional processing. But in fact by defining the level of measurement of the variable as categorical we could be preventing the algorithm to apply useful mathematical operations that suit better the nature of a time-related variable.This concerns only to the results I am evaluating: probably using other tools would ask for a different approach.3) Other problem that I had wasscaling or not scaling?Transformation or not transformation? I used scaling (read: normalization) for the weather variables which didn't result in clear improvements in performance and sometimes worsened the results (I am talking about R, haven't tried for New Bayesians yet). And although the correlation between the scaled variable and the original one was 1.0, none of the weather variables are gaussians, so perhaps normalization scaling was not appropriate.EDIT: checkthis linkI found in the thread started by a New Bayesians participant.The same goes for transforming the data (for example, log transformation of the dependent variables). Transformation of data seemed to be working for some (I am not listing now...) but it may not be ok for random forest according to my last results.On the other hand, many other people are getting improvements with transforming the dep-variables.No-one has mentioned scaling yet.Due to the number of trials, the way I worked it and the mixed results getting by different people it wouldn't be responsible to totally affirm that scaling/transformation wasn't good decision. Nevertheless as I found in several external links, scaling/transformation always deserves some discussion. Perhaps the most single advise I could suggest was related to an external link mentioning that scaling is advisable if the plan is to evaluate possible interactions or if the variables were to be powered. Have a look and let me know?4)For those using R:caret or not caret? Be also aware that my first interest was the performance ofRecursive Feature Elimination(RFE) in the caret package, using rfFuncs. I had previous good experience using this method and the package. In fact, RFE was good enough to take me under the .43 benchmark (similar to the most recent result using randomForest package), a result that differed substantially from the catastrophic application of the caret::train with rf as method. Unfortunately I didn't keep a tidy follow up of my code for this project and I lost my best performance script, so I cannot say what worked (apologies for this...).But if you are planning to give it a try, be aware that:It was SLOW, because I was doing validation and particularly RFE visits A LOT of solutions before selecting for you the features so let alone would take long timeIt was not stableI am not suggesting that caret is a bad option: it would be a mistake to generalise from my experience, although I must admit that I am more likely to speed up my phasing into more scikit-learn after so much waiting and frustration.EDIT (28-05-2014):Why caret is still a very useful tool? Because it is a good package for setting up many different analyses with proper validation, grid search, metrics, etc. With a little work you can change the defaults with customised functions. Additionally it can be parallelised. And R is in my opinion very powerful in reporting and visualisation.5) Thebenchmark given by kaggle people for this project is benignin my opinion. A more honest one could be to start measuring from the RMLSE obtained by averaging all values of casual/registered per month (around .56, according to Gautam Gogoi's commenthere). That means to raise the bar up.On the other hand, although we have to respect the pecking order of \"the first to arrive wins\", I also must say that for a competition designed to learn an extra-official way to fairly evaluate your personal performance is to evaluate the performance without the date of arriving to that solution, particularly if there are several getting the same solution. In short, no matter how long did it take you and how, the most important is at least you got it.Finally, be aware that our results depends of other things (like using 64bit instead of 32bit machines), so don't feel disappointed if someone takes the lead for a few points...---------------------------------BONUS LINK--- Like the answer to this question:http://www.kaggle.com/c/bike-sharing-demand/forums/t/11441/better-then-0-40-score-solution-in-r-language--- One about using random forest with this data and about data mining in general:http://stats.stackexchange.com/questions/112148/when-to-avoid-random-forest--- Other ones about the effect of indiscriminate application of tools and data analysis without good understanding of what (about big data):http://www.economist.com/blogs/economist-explains/2014/04/economist-explains-10#commentshttp://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts Hi kagglers!This is the last in a series of analysis of RF applied to this problem, subject to the \"prior data\" constrain, where I am also making trials in sk-learn and R.Method:The models I used for comparison were:- max 40 days prior complete data- log10 of casual or registered (\"\") or not (\"nolog\")- data is numeric (as New Bayesians first public model)- 1 trial each- either introducing lags of weather data (\"noisy model\") or not (\"clean model\")My results:caret train rf, noisy model nolog ---------------------> .558randomForest using all variables, noisy model nolog ---> .607caret train rf, clean model  nolog --------------------> .529randomForest using all variables, clean model nolog ---> (ND)caret train rf, noisy model ---------------------------> .503randomForest using all variables, noisy model ---------> .552caret train rf, clean model ---------------------------> .495randomForest using all variables, clean model ---------> .496In this case, it seems that caret train is doing what it is meant to: validating and selecting the best model for the RF method when some noise still exists.Compared to sk-learn? The clean model only:sk-learn RF regressor clean model ---------------------> .495It is unclear how sk-learn RF regressor would have responded if a noisy model would have been passed and in particular how that would have compared to R randomForest or caret rf. Although I cannot claim about any differences in final outcome, I like sk-learn more, and I just like python more anyway...Next...In principle I would like to progress by improving/re-factoring this code, involving other techniques different to RF to evaluate the same problem, eg. other Bayesian techniques,  GBM/GBT, regularised regressions, etc. If I have time I would also compare some R and sk-learn performances.------------------------------------BONUS LINK:Bayesian Methods for Hackers Thanks a lot for the quick reply. Yes that helps a lot! Got it working Hi Startlight,You could create a new summary function. There are several links on internet about it. In particular, start by creating a nested function:myRMSLE <- function (data, lev = NULL, model = NULL){if (is.character(data$obs))data$obs <- factor(data$obs, levels = lev)mypostResample<- function (pred, obs){isNA <- is.na(pred)pred <- pred[!isNA]obs <- obs[!isNA]msle <- mean((log(pred+1) - log(obs+1))^2)out <- sqrt(msle)names(out) <- c(\"RMSLE\")if (any(is.nan(out)))out[is.nan(out)] <- NAout}mypostResample(data[, \"pred\"], data[, \"obs\"])}And then when calling control, call your function explicitly:ctrl <- trainControl(method ='repeatedcv', number=3, repeats=5,summaryFunction=myRMSLE)Don't forget to specify also the function at \"train\":modrf <- train(registered+casual ~ y+s2+s+dw+t+h+wd+tp+atp+hum+ws+w, data = trainingDrf1, method = \"rf\", trControl = ctrl,metric=\"RMSLE\", maximize = FALSE, verbose = FALSE)There is likely other approaches to this one that I have seen but I don't have links by now, sorry.Hope this helps. Thanks for the information. How did you do the following?I managed to modified caret (yes, still using it and getting better on it...) to measure RMSLE instead of RMSE and despite of the fact that RF is not advised for time series (see one of previous notes), I decided to give it a try just to explore the data Nice find. I think the (near) future will see a lot more graphical and real time machine learning tools, as the field is becoming more and more popular. http://blog.dato.com/using-gradient-boosted-trees-to-predict-bike-sharing-demandhttp://nbviewer.ipython.org/urls/s3.amazonaws.com/datarobotblog/notebooks/gbm-tutorial.ipynb Hi Kagglers!Now, by repeating the same approach (i.e. using 40 days of prior complete data to predict the non-observed 10), using a similar model as exposed above and Random Forest (R, without caret) as exploring method BUT transforming the dep-var by log10 (instead of ln as it is apparently the common practice), I managed to cross under the .50 limit - just! This beats my best caret rf 0.503 with log10 and lag/for variables (read more about it below).Part of the trick seems to rest on the transform. In a very interesting public analysis by Jay Gu (thanks, Jay!), he appliesGradient Boosted Trees(python's graphlab library) to solve the \"using all data\" approach of this problem. He also mentions that it was good to transform because the measuring was based on RMSLE and not RMSE. I think that another relevant impact of the transformation in this case iscontrolling for outliers, skewness and or heteroscedasticity. There is enough of all that to affect the precision when working non-transformed dep-var's, but by packing the deviating points into a transform they become part of the crowd. Why I think that the transformation is helping to control for deviation is based on the fact that many have been using Jay's transformation (ln), but Jin L (seePython - Scikit-learn based Random Forestthread) used log10 with excellent results for the case of using all data to predict the non-observed data. This could evidence that the transform selection should not been seen as measuring setup as Jay claimed, but it should rather be seen as a deviation control. Jay's claim should be taken in consideration though.What hasn't worked? In theRulesthread some people like S. Beale and ViennaMike commented they tried to experience with bringing back and for weather data. I was doing a few trials before I read their notes but after reading I put more interest on it. So far adding lagged weather data seems to pollute the random forest // --- this sounds an environmentalist claim, and probably it is! --- //. It seems to exist some collinearity and or redundancy when using the back/for information that could affect the performance of the method. I did a simple correlation test to select my lagged variables and no additional transformation.Anyway, let me know if someone has a better result?---------------------------------------BONUS LINK: Other links in this forum presenting/discussing models using only prior data - apart of the \"Rules\" thread:http://www.kaggle.com/c/bike-sharing-demand/forums/t/10968/performance-using-data-prior-to-prediction-datehttp://www.kaggle.com/c/bike-sharing-demand/forums/t/12534/any-luck-with-time-series Hi kagglers!(IMPORTANT NOTE! read the discussion about the topic of this postin this thread; kaggle coordinators were involved; by the time I wrote this thread I was not certain if that was a rule... Edited to reflect the fact that using prior data to the rental is actually a rule)Today I have been exploring the concern of some of us about \"using prior data to the rental\".In principle, I don't know still if that is a rule or even a hint!, the fact is that I decided to make a simple exploration under the assumption that we don't know what would happen after the non-observed point of rental period.I managed to modified caret (yes, still using it and getting better on it...) to measure RMSLE instead of RMSE and despite of the fact that RF is not advised for time series (see one of previous notes), I decided to give it a try just to explore the data.The model was again that with the best results as above: the model suggested to date by New Bayesians (Shubham).There was not improvement,with partials of RMSLE's around .50 when using 40 days of complete (observed) prior data as reference, a bit under .53 at leaderboard. Again,this is using caret::train::rf, which didn't give me good results in the first trials.I wonder if at least it can help me to identify a trend I could eventually use for a better result though... In fact, this result was better than using caret when the assumption was not considered... butI haven't tried the random forest alone yet...EDIT: randomForest without caret (fixed values for parameters ntrees and mtry as best rf performance) apparently not a good idea...Regarding the condition (\"prior data...\") I still think the proposal is worth exploring: in fact, this seems to be a more real-life predictive exercise.As someone already mentioned in another thread, we won't be able to make easy comparisons between our results as even myself was using all data (and actually, I will keep doing so now and then...).  In general, despite the fact of lack of comparison between kagglers, at least there is the chance of individually comparing the results. And there are real situations in which using the full data is also possible, so I don't think it is wrong exercising with it.Still, it would be great if you can report your achievements for this particular case! I will see who more is talking about it and let you know if I found more...Let's keep working!----------------------------------BONUS LINK: For those interested in marketinghttp://www.insites-consulting.com/big-data-is-the-end-of-the-beginning/",
    "As many of you I am a python fan and for this project (and for many others...) I stuck to python's pandas as my base language for data manipulation.But I have been troubling myself when selecting an analytical tool that make me feel comfortable. What is the most effective and easiest way to analyse data from python? I would like a tool  that  renders simple but powerful functionalities and that are flexible enough for coding in, let's say, batch projects, looking for some level of scalability.I tried something as simple as a linear regression for comparison. I visited sklearn's  LinearRegression, then statsmodels' lm and I liked the last one but it gave me again a dejá vu of R, the language I was using previously.I don't see R so scalable as python and it lacks of the cleaner code, vectorisation and memory management that python provides, but R's style makes sometimes your life easier offering self-explained syntaxes and caring functionalities. For example: by implementing the Formula expression and having a built-in function for manipulating categorical variables, R 'guesses' your manipulation requirements and extend that to other dataframes if required.You can argue that stadsmodels via patsy provides a similar effect but I really wonder why to use a substitute of the original idea, considering that statsmodels 'clones' R in many ways without necessarily being so powerful.So, keeping consistency in the use of base language I have found rpy2  an interesting option. A quick look of part of my code converting pandas into R format and then using R \"lm\" functionality through rpy2 libraries:data_trainr = com.convert_to_r_dataframe(data_train)data_testr = com.convert_to_r_dataframe(data_test)lmod = ro.r.lm(\"log(casual+1) ~ dm + t + wd + tp + hum + ws\", data = data_trainr)numpy.array(ro.r.predict(lmod, data_testr))Where \"ro.r\" is a rpy2 object and \"data_*\" is pandas df.Issues? Always! To mention some of them:- For this specific case if you are creating R objects into the python environment, calling reports like the default summary.lm will work but will also show substantial amount of undesired information. This is because the summary function in this case is translating the information sent through python into the summary function as A FUNCTION, and that includes the dataset, to mention some. When working natively in R, the summary functionality identify the dataset as a different R object though. The problem is that R doesn't have an easy way to customise that summary so a work-around is to bring the python dataset into the R environment itself where R will treat it as native R object from start, but then the code from python should be as command sent from a sort of eval line, like it is done with many other drivers. Eg.: call the Environment of the R process in use and save the file as a variable in that environment:datat_dfr = com.convert_to_r_dataframe(datat_df)ro.globalenv['datat_dfr'] = datat_dfrprint(ro.r(\"summary(datat_dfr)\"))Additionally, you can also parse transformed elements and objects using the % text formatting, but that could pollute your code too.- When creating r objects using python you are duplicating the datasets, but this is somehow similar to create new variables or even datasets from pandas to sklearn or statsmodels format for analysis- There are numerous discrepancies in the way both languages work the indexing or even the call for classes and attributes. This makes hard to figure out how to call some functionalities from R using the rpy2 interface when, for example, a '$' instead of a '.' should be used. Calls is not a fundamental issue when working with python libraries, although it is somehow surprising the existing discrepancies for indexing and the confusing redundancies when using python libraries (try numpy vs statsmodels vs pandas... it can bring you mad!)- Other things: rpy2 is expected to be slower during any communication between python and the R process/tread if the low level of programming is not being used; can be unstable; the code can become longer and, without the right care, unreadable; for coding natively in python you MUST know how to call r functions (although some ggplot2 examples are encouraging...!); I don't know if changes in the R environment will be correctly adopted if coding from python natively; and in general it is not by any means complete, with several bugsDespite of it I really like this project though. So far I see some potential combining the programming power of python with the several things where R surpasses python's implementations, or just having an additional weapon by hand without needing to change languages. If you are interested, here some links:http://rpy.sourceforge.net/rpy2/doc-dev/html/introduction.htmlhttp://pandas.pydata.org/pandas-docs/dev/r_interface.htmlEDIT29-May-15:Forgot to mention: documentation could be brief and examples are not everywhere, although the developers have been really responsive to questionsAnd for a caret use from python environment:import pandas.rpy.common as comimport rpy2import rpy2.robjects as rofrom rpy2.robjects import Formulafrom rpy2.robjects.packages import importrcaretr = importr(\"caret\")data_trainr = com.convert_to_r_dataframe(data_train)param1 = {'method' : 'repeatedcv', 'number' : 3, 'repeats' : 5}ctrl = caretr.trainControl(**param1)param2 = {'method' : 'rf', 'trControl' : ctrl}rf_for = Formula(\"log(casual + 1) ~ dm + t + wd + tp + hum + ws\")rfmod = caretr.train(rf_for, data = data_trainr, **param2)print(rfmod)",
    "I used Negative Binomial Regression and got 0.7 deviation, which is my best. I know random forest can bring me up to around 0.4, but still want to know if there is anyone has achieve good score using regression. Thanks in advance for sharing!",
    "After quickly checking the data I decided to play around with the following approach:- Zero-inflated Count Data Model- use splines (smoothing) for continious predictors- use year, month, wday, hour as factors- use interaction variablesI was able to improve my prediction score slightly below 0.5.However, my code / modeling approach is really intensive from a computational perspective.https://github.com/ramialfahham/bike-sharing-demand Thanks",
    "The total number of data in the training data set is not a multiple of 24? Is any data missing? Tian You got it right. The missing data (in both training and test sets) are the hours when both registered and casual counts are zero. I guess this happened at the time of counting the totals by hour. A typical dbase program does not return the instances of zero totals. For model fitting, the missing data (where we know for sure that the counts are zero) this would cause some bias in predicting the counts for early hours (say around 4 to 6 AM). Though statistically no sound, the missing data would not cause any serious issues with predicting the total demand over the network. It is indeed missing some date-time. But as it is test data and not train data, we don't have to concern with completeness of data Does anyone know how to fill in the missing data points with let's 'zero' values? But for this the other variables need also be 'interpolated'. Anyone who can help me code this in R? I think they left out all the timestamps where the counts were 0... I also noticed that. And actually I think I am really unhappy about that....abhishek mishra wroteThe total number of data in the training data set is not a multiple of 24? Is any data missing?",
    "Howdy y'all,Since Brandon Harris's model was well received, I thought I'd share a simple neural network in R that gets you into the top third, or at least got you into the top third when I published it on my blog a few weeks ago.http://www.evanvanness.com/post/100217670076/neuralnet-r-package-neural-network-to-predict-kaggle I had the same problem. After using str(testmat) I noticed I had 37 variables, although I had never changed any variable. But it worked after changing into 37. Thanks! threshold=.04,stepmax=1e+06,learningrate=.001,algorithm=“rprop+”,lifesign=“full”,likelihood=Tfit <- neuralnet(formula,data=\"train.mat,hidden=c(7,8,9,8,7),threshold=.04,\" stepmax=\"1e+06,\" learningrate=\"\"> Thank you for the explanation. It is very useful for me :D Hey Epid,1) You can find the documentation for model.matrix fairly readily ongoogleor by using the?model.matrixcommand in R Studio.2) We scale count because we don't train neural networks on integers, but rather numbers in [0,1]. It may be helpful  to think of these numbers as a measure of \"activation response\" to a particular input. You can learn more about the basics of neural networkshereandhere.3) Let's look at the command:fit <- neuralnet(formula,data=train.mat,hidden=c(7,8,9,8,7),threshold=.04, stepmax=1e+06, learningrate=.001,algorithm='rprop+',lifesign='full',likelihood=T)The vectorhiddenis a 5x1 matrix. The value of each row denotes the number of nodes in that particular layer. Thethresholdparameter is used to determine when the neuralnet algorithm stops. If you want to know exactly how this is determined you can consult the documentation.Stepmaxsimply caps the level of computational resources available to the algorithm.Learningratedetermines how much each observation affects node weighting.Algorithmdetermines the way the neural network learns. If you want to learn more about rprop+, you should readthis paper.Lifesigntells r how much to display during the calculation.Likelihoodis for when the log-likelihood is negative. Hi EvanVanNess,Thank you for your tutorial, may I ask some questions,I am a newbie :D1) Can you explain howmodel.matrixwork?and why do we need to convert the variables into matrices for neuralnet?2) Why do we need to scale count?3) How do we know that the network has 5 hidden layers and can you explain other parameters in the formula?threshold=.04,stepmax=1e+06,learningrate=.001,algorithm=“rprop+”,lifesign=“full”,likelihood=TThank you very much!!! Starlight wrotepredict<- predict$net.resultThanks for this.  Fixed it now.  It's crazy how many errors got added when I published it. try it and see if it works!  (it won't)will write more later when I have chance. Evan,Meant to get back to you before now, but had finals all week.  Thanks for your quick reply, your answer helped me understand the problem better.One question however, couldn't you get a more complete answer if you had two output nodes?  I am thinking one for count, and the other for either casual or registered.  I am suggesting either one or the other since once you get one, the other is a simple subtraction.  If you think I am on to something, could you help me understand a way to get two output nodes. Good question, and one I'm surprised others haven't asked.Honest answer?  I figured it would look more impressive if it had more layers  :)You can get the same results with less layers, but using a few more doesn't use much more computation time, so I figured I'd add a few! I am a noob at this and while this question might seem simple to everyone, I would like to know why did you use 7, 8, 9, 8, 7 for your hidden function?By the way thanks for the code Evan, got it to work with little effort. :-) Hi,I am amit paranjpe and i want to build this project in Base sas and Data integrationcan anybody help me in getting this project done in base sas and data integrationI have just completed my certification in base sas bi and data integrationThanks and Regards,Amit Paranjpe Hi,I've been getting a ton of trying to build a neural net in the way you Thanks for the example.A small type in the example:#Assign predictions to variable because compute produces more than we needpredict<- predict$net.resultinstead ofpredict<- predict$net.rsult Thanks Evan.Worked perfect after a few minor adjustments. Does the model account for the fact that only information prior to the rental period can be used ? I had 37 columns also. With exactly the provided code the score on the training data is .411 and on the test data is .500. These are worse than the scores I got using Brandon's model (and tweaks thereof).What scores did you folks get? I ask to see if I could have made some mistake since we should all get the same score with the same code.PS. There is a mistake in Brandon's model by the way in that he creates a variable Sunday but the way his code is written all values are set to 0. Surprisingly once I corrected that, the score went 'up' (i.e. got worse). DarkMatterB --Did you change the model at all?  Particularly did you change the variables?The error is happening in:testmat <- testmat[,2:38]Error in `[.data.frame`(testmat, , 2:38) : undefined columns selectedRun all the code up to that point in R.  then enter str(testmat).   If you changed some of the variables then you will see a different number of columns than 38.   Change the 38 in the code to your number of columns.Elnaz -- thanks for reporting back.  Glad it worked!  Good luck! Great! Yes the error was because of  the quotation marks. Thank you. This what my console looks like after building the network, everything appears to work until then:hidden: 7, 8, 9, 8, 7 thresh: 0.04 rep: 1/1 steps: 1000 min thresh: 0.091091235322000 min thresh: 0.048482849323000 min thresh: 0.048482849324000 min thresh: 0.048482849325000 min thresh: 0.048482849326000 min thresh: 0.048482849327000 min thresh: 0.048482849328000 min thresh: 0.045446113279000 min thresh: 0.0418942292510000 min thresh: 0.0418942292510337 error: 14.86185 aic: 1139.72369 bic: 5188.57792 time: 5.69 mins>> ## Delete the first column. With the neural net package, you have to be careful to always make sure that the covariate matrix matches your test set. Columns must be in the same order.>> testmat <- testmat[,2:38]Error in `[.data.frame`(testmat, , 2:38) : undefined columns selected>> #Get predictions>> predict <- compute(fit,testmat)Error in neurons[[i]] %*% weights[[i]] : non-conformable arguments>> #Assign predictions to variable because compute produces more than we need>> predict<- predict$net.rsultError in predict$net.rsult : object of type 'closure' is not subsettable>> #Rescale>> predict<- predict*1000Error in predict * 1000 : non-numeric argument to binary operator>>> #Check for any negative variables>> predict[predict<3]Error in predict < 3 :comparison (3) is possible only for atomic and list types>> # We’ll set the minimum prediction here to 3.8>> predict[predict<3] <- 3.8Error in predict < 3 :comparison (3) is possible only for atomic and list types>> submit <- data.frame(datetime = test$datetime, count=predict)Error in data.frame(datetime = test$datetime, count = predict) :arguments imply differing number of rows: 6493, 0> I can think of a few possible errors to start with:1.  the quotation marks are getting turned from \" to ”.   Make sure they are just the two straight lines instead of the two curved lines.2.   your formula isn't named formula3.  your data isn't named trainmatalso make sure you've got the neuralnet library installed and loaded. I'm receiving an error while running this code:fit <- neuralnet(formula,data=trainmat,hidden=c(7,8,9,8,7),threshold=.04,stepmax=1e+06,learningrate=.001,algorithm=”rprop+”,lifesign=”full”,likelihood=T)The error says unexpected input in \"fit.... What errors are you getting?   The code was implemented in RStudio 0.98 I've been getting a ton of errors trying to build a neural net in the way you describe. Did you have success with this in R? or R Studio",
    "Hi kagglers,I would like to know if some of you are playing with different languages and tools to investigate the same problem and obtaining still different results?I started using random forest using R's caret package (either training or within recursive feature elimination with rfFuncs) and after some frustrating results where even couldn't get replication of my best results (this is possibly my own fault...), I decided to use Python scikit-learn implementation by \"New Bayesians\" (thanks, Shubham!!!) for a comparison.First of all, I would expect from the caret package to be more accure: in the code made public by Shubham there was no validation (unless I am missing something...), something that caret functionality does have.  However, when using New Bayesians model only once I got even a better score than any of my trials using R. When trying to take some aspect of their model and using R packages I still got very bad results, sometimes 30 points worst, despite of obtaining RMLSE's at validation stage that were similar what you get with New Bayesians code (around .407). In fact my results using New Bayesian model with caret::rfe or caret::train were many times worst of what I managed to get previously with my best R script (under .43).Ignoring any possible mistake I made with the script (it is always possible...), I still wonder if anyone is trying to replicate the same results using different languages and packages and getting huge discrepancies?Additionally, although it is clear that the validation procedure is numerically intensive the R implementation is always too slow, sometimes taking more than 1 hour (if you want to replicate, be aware...). I am connecting to R through python by using the rpy2 library, but I don't think that that is the problem: as soon as it starts using two cores in a multicore implementation the R processes take control, so I guess it is slow only when python calls R. I found several references also in this competition suggesting that R implementations are taking a lot of time.So, is anyone experiencing large differences between RMLSE at validation and RMLSE at test when using rfe or training (caret::) with R? What about other implementations of random forest (eg randomForest) using R? And how long does it take?To facilitate the comparison, I would suggest to verify the following equation in all comparisons:(casual or registered) ~ day + time + year + season + holiday + workingday + weather + temp + atemp + humidity + windspeedwithout factoring the variables and without log transformation of dep-var.Looking forward to your feedbacks, kagglers! Hi Kagglers,A note about python's pandas / sklearn / statsmodels (via patsy) and R through python using rpy2.I will be using a linear regression as a test for one experiment I am doing and I was testing which could be the best and easiest way to approach the problem keeping the base language as a python form, essentially pandas. Here a quick view of some of my observations.First of all, I have commented about differences in the algorithms between languages. Well, guess what! They definitively exists for some of the functions.One particular case is the linear regression: while in statsmodels (and probably also sklearn) implement a Singular Value Decomposition (SVD) for the linear model fitting, R is set to use the QR decomposition at lm.fit. SVD could be ok when dealing with large amount of data for stability but a quick algorithmic implementation should be in place. SVD is said to be more numerically intensive but more stable that QR. And yes: some references argue that there are some differences in the results at certain situations between both (I am not aware of it). Matlab could implement both (according to their references).",
    "Helloi study statistics and machine learning, i use frequently R and python, i m new in kaggle, i would like to build up a team to share our knowledge and work on competitions. if your interested too, please conatct me at : zinounemed@hotmail.com.",
    "Hello!I recently made adetailed tutorialabout using R inside of Azure ML with this dataset. I did some feature engineering in R andAzure ML Studio is a user interface that makes data science drag and drop! It's great for those of us who aren't software engineers. You can get a free trial of Azure ML and download the completed model I madehere.- Evan",
    "Has anyone had any luck with time-series approaches?  I've tried using the first 19 day of the month for training but my best models score around 0.95 whereas using a GLM is scoring in the 50's.  I am not sure why it is performing so poorly.However, my GLM model is sort of 'cheating' since I am using all of the data.  I just wanted to see how good of a prediction was possible.Thanks. Hey Robert, interesting approach.  I am curious if you have tried comparing your approach to a simple 24 separate linear models and to the lookup table only model.I've tried using autoregressive models and simple exponential smoothing models trained on the first 19 days of the month, so, 24x2=48 models total and it gives me a leaderboard score of around 0.95.  I square-rooted the response since that seemed to make the histogram look normal and performed a little better than the untransformed response.I am curious as to why it is performing so poorly since the trend seems relatively stable. James, my best model so far is using linear models to \"mop up\" the residuals from a look-up table indexed by hour of day and day type (which appear to be the two strongest predictors). There are 24 linear models, one for each month, which are trained on training data only from the past at the time of prediction; specifically, not more than 60 days old. If you are constructing GLM's, I guess you would have to use some kind of similar scheme to avoid training on data that is from the future at the time of prediction.By time series model, perhaps you mean an autoregressive model? If so, my experience with such models is that they wander away as the time to prediction gets more and more distant from the most recent known value. If not, maybe you can explain what kind of model you are working with.best, Robert.",
    "I don't see any temperatures below zero degrees centigrade in the data set.  Am I misreading the data or is global warming coming faster than predicted? From the UCI Machine Learning repository, where this data is hosted - https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Datasettempis described as Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale) According to weather.comhttp://www.weather.com/weather/wxclimatology/monthly/graph/USDC0001theaveragedaily low temperature during January in Washington DC is -2 degrees centigrade (and -1 during February).  The record low temperature is -26 degrees centigrade. Did you notice that this dataset obtained In Washington? Yes, the temp and atemp variables make no sense...they are certainly WRONG. So...what kind of conversion could they be using??? They certainly are not converting to Fahrenheit. I noticed the temperature variable goes up in increments of 0.82, so I think it has been converted. I checked weather history here:http://www.wunderground.com/history/airport/KDCA/2012/7/7/DailyHistory.html?req_city=NA&req_state=NA&req_statename=NAComparing, to the \"temp\" data provided, if you really want to know what the true temperature is (not that it would affect your methodology), you could use the following:Fahrenheit = 2.1293(temp provided) + 17.242",
    "Hi,I just created a tutorial (Bike Sharing). Hope some of you may find this useful.http://technospeaknow.blogspot.in/2015/02/prediction-of-registered-bike-sharing.html",
    "Hi guys.Looking for teammate for participation in Bike Sharing competition - sharing thoughts, ideas, improving score to top 10%. I am currently in top 20% with RF and result 0.45. Work in R.Pls contact me via skype: sergey_mmm",
    "Why is the test set the end of the month?  Shouldn't the test set be a random set of data since testing on the end of the month can lead to bias? You have to predict bike rentals for the end of the month, since 20th day What I mean to ask is for the final evaluation on the hidden solution, are we trying to predict the bike retals at the end of the month or will it be prediction for the full month? Or a random sample of rental data?",
    "Hi I am getting the error while submitting thesubmit.csv  file.> head(submit)datetime count1 2011-01-20 00:00:00 1172 2011-01-20 01:00:00 363 2011-01-20 02:00:00 364 2011-01-20 03:00:00 655 2011-01-20 04:00:00 656 2011-01-20 05:00:00 135>excel :datetime count2011-01-20 00:00:00 1172011-01-20 01:00:00 362011-01-20 02:00:00 362011-01-20 03:00:00 652011-01-20 04:00:00 65Error screen shot in attachment.I don't have column 21 and negative numbers in my output file.data type :> str(submit)'data.frame': 6493 obs. of 2 variables:$ datetime: chr \"2011-01-20 00:00:00\" \"2011-01-20 01:00:00\" \"2011-01-20 02:00:00\" \"2011-01-20 03:00:00\" ...$ count : int 117 36 36 65 65 135 116 103 113 98 ...>Pls help HiI think you use linear regression and for values that should be close to 0 it predicts negative values what is impossible - it is impossible to have (-10) rented bikes. So you see error that -10 in not in the required set of value [0 to infinity] hi,my guess would be that there is some mistake in your file format (since it seems that your dates are read as numbers) - try opening your submission file in a text editor (e.g. Notepad ++) and compare it to the sample submission. If that doesn't help please post a screenshot of the file in Notepad or just a part of your actual submission file.best regardsValentin",
    "Hello Everyone,I am new in Data Science, from last one month i am active on kaggle.I have knowledge of Machine Learning & Python, Learning R now these days.I am looking for team mate, If some one interested please contact me 1991manish.kumar@gmail.com Hi everyone. I'm not new to data science but I would love to connect with others to trade insights.I program in R almost exclusively and I will share my work on Github. My Github username is feelosophy13.If anybody's interested, please let me know. My email address is feelosophy13@gmail.com. Hi Manish,I am also new in data science and looking for team mate. I am learning R language from 6 months. If you are interested please accept my invitation.Thanks, send invitation I'm also looking for teammates and I'll be glad to work with you. Hey Manish,I'm looking for people to collaborate as well. Contact me shulhi@gmail.com",
    "I have a submission error and I can understand why. I m new to here I just want to try out @razgon's method that posted in gbm.I will be appreciated if someone help me. I tried in excel too, it is not working for me.What should I do? Thanks, I have used the same format yyyy-mm-dd hh:mm:ss and this worked for me:)Best regards,Ritesh Hi,The format of the datetime field for the submission has to be exactly yyyy-mm-dd hh:mm:ssex. 2011-01-20 01:00:00It worked for meBest I have the same error problems, and I used the \"sample submission\" file from the Kaggle site. Can someone explain why it does not work?, What should the string look like? The \"unable to find a key value\" error means that the scoring system is not finding exact matches for one or more of the ids (unique elements) in the key column (in this case, the datetime). Check your submission in a text editor to see why the strings are not matching. Getting the same error even after converting it to character type. Thank you EC. I will try it again. I had a similar problem. Try converting the datetime variable to 'character' before you write the CSV and then resubmit.",
    "Here's what I encountered.After submission, it said something like cannot find value \"2011-01-20 00:00:00\" in datetime column. I checked it is in the format of \"2011/01/20 00:00\". Is it the reason why I got error message?Thank you guys! format in Excelyyyy-mm-dd hh:mm:ssHTH I tried using excel too, it is not working in my case. What to do in this case? Hi Hong,Thnks for sharing the format to resolve the submission error . Solved  Tnks! Hello Hong  ,  I  faced  the next  error :ERROR: Unable to find 6493 required key values in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 00:00:00' in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 01:00:00' in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 02:00:00' in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 03:00:00' in the 'datetime' column ERROR:Could  you tell me   what  have  you done  formatting?Tnks Problem solved! Reformat in Excel. :)",
    "Hi, I am getting the attached error while submitting my solution. I have not changed the format of datetime column and even tried submitting the solution using sample submission file, only to receive the same error everytime.Can any one help please what could be the reason? Tarun,Please open the sample submission file as a txt doc and look the format. You should have the same format on the submission file also. Some times you may store the details with \\r than \\n. The below link has some info on thathttp://nicercode.github.io/blog/2013-04-30-excel-and-line-endings/Tinto Your dates are formatted as \"1/20/2011\"",
    "Hey, thought this would be more fun to work together with some people. If you are interested in working together please respond to this with some form of contact I also want to join. I am trying to do it in R. I got some ideas. But it would be better to discuss. My email id is vinayprakash808@gmail.com . Please get back hassiktir wroteHey I emailed those of you who left your email hu_xq2014@163.com I am also interested in starting a team. I have worked in R and python before doing predictive modeling. ScDavis6@gmail.com Hello,I would like to join your team as well. I have experience in predictive modelling.dada.kishore@gmail.com You can add me too: gideonaina@yahoo.com Add me in - koye.sodipo@gmail.com Helloadd me to team, my email aaizz.zeroual@gmail.com I am very interested in joining a team ,do ping me at@sujithkumar446@gmail.com Are you guys still interested? I would like to team up. My email: lei.gong@outlook.com hello i am sujith, i am also searching for a team, can we be if so mail me at sujithkumar446@gmail.com Hello, me too I would like to team up. I've looking at the problem for some time. Would be nice to have an exchange and work together. My email: fcanomar@gmail.com Hi, I'm a bit late but I would like to join the team as well. My email address is feelosophy13@gmail.com. Hello Maxime,i will be interested. My e-mail is jp.gossuin@gmail.com expect to join into a team , my name is kobe ,  kobe_mak@163.com Hey, here is my email: morellemaxime@gmail.com :-) Hey I emailed those of you who left your email Hello Maxime, I interest but couldn't send you PM, you can send me an e-mail: gokcergun@gmail.com Hello everyone, is there still someone intersted in forming a team?If so, feel free to PM me :-)Regards. very interested! Hi, please add me to the team,ofigue@gmail.comThanks Can I join too? I would like to join!yhansjung@gmail.com I would like to join if you have spots left Of course!!My email address:imhenry@me.com.Please add me in! Hey if you guys want to leave an email I will include you in the emails and we can figure out a course of action for the team.",
    "Hi everyone,I'm trying to make a submission in Kaggle, but for some reason I get the following error. Does anybody know how to solve this?The value 'NA' in the key column 'datetime' has already been defined (Line 3871, Column 1)Thanks! Open your submission in a text editor. Look near line 3871. Instead of a unique identifying \"datetime\" like \"2012-03-25 01:00:00\" you probably have a NA value there. Download the SampleSubmission from the data page and compare the first column \"datetime\" with your submission. They should be exactly the same (though the order does not necessarily matter).",
    "Here is a solution for Bike Sharing Demand forecast problem using Random Forests  in R - http://www.techdreams.org/programming/solving-kaggles-bike-sharing-demand-machine-learning-problem/9343-20140821With this approach it could get RMSLE of 0.70 which placed to somewhere close to the mid of leader board, but could not improve it. Hoping to working with others to improve this code. I forgot to mention year , soprediction = mean for (year, day_of_week, month and hour)So this submission even satisfies the demand to use prior data also.Below is my code and the submission will get you a score of around .56---------------------------------------setwd (\"C:\\\\Kaggle\\\\Bycycle Sharing\")train <- read.csv(\"train.csv\")# Create factor variablestrain[,2] <- as.factor(train[,2])train[,3] <- as.factor(train[,3])train[,4] <- as.factor(train[,4])# Creating timeseries and datetime objecttrain$datetime <- strptime (train$datetime , \"%F %T\")train$month <- format(train$datetime , \"%m\")train$month <- as.factor(train$month)train$hour <- format(train$datetime , \"%k\")train$hour <- as.factor(train$hour)train$year <- format(train$datetime , \"%y\")train$year <- as.factor(train$year)train$day_of_week <- format(train$datetime , \"%u\")train$day_of_week <- as.factor(train$day_of_week)test <- read.csv(\"test.csv\")test[,2] <- as.factor(test[,2])test[,3] <- as.factor(test[,3])test[,4] <- as.factor(test[,4])test$datetime <- as.POSIXct(test$datetime)test$datetime <- strptime (test$datetime , \"%F %T\")test$month <- format(test$datetime , \"%m\")test$month <- as.factor(test$month)test$hour <- format(test$datetime , \"%k\")test$hour <- as.factor(test$hour)test$year <- format(test$datetime , \"%y\")test$year <- as.factor(test$year)test$day_of_week <- format(test$datetime , \"%u\")test$day_of_week <- as.factor(test$day_of_week)# SQLdf and prediction based on mean for year, month, time and day of weeklibrary(sqldf)train$datetime <- as.POSIXct(train$datetime)train_mean <- sqldf(\"select avg(count) avg_count, avg(casual) avg_casual, avg(registered) avg_registered, month, year, hour, day_of_week from traingroup by month, year, hour, day_of_week\")# Join for prediction - Model 1prediction_1 <- sqldf(\"select b.datetime , a.avg_count from train_mean a, test b where  a.month = b.month  and a.year=b.year and a.hour=b.hourand a.day_of_week = b.day_of_week \")names(prediction_1) <- c(\"datetime\", \"count\")write.csv(prediction_1 , file= \"submission_25.csv\" , row.names=FALSE)------------------------------------------ I dont have too many references regarding variable creation, but you can check out this blogpost on kaggle which touches upon that. -http://blog.kaggle.com/2014/08/01/learning-from-the-best/Also variable creation depends mostly on domain knowledge and can also be spotted by doing an EDA. There are no set rules for variable creation and it completely depends on the problem at hand. Am not able to recollect an example of vaariable creation / feature engineering but if you check the previous kaggle competitions you would understand how important it is. Let me clarify this with an example:suppose we want to predict the total count of bikes rented each hour for 21st of January 2011. If we use all the training data (which runs till 19th of December 2012) to build our model that means we use information from future to predict the count of bikes rented on 21st of January 2011.The correct way to do this is building a model based on the information prior to 21st of January 2011.For example if we want to predict the count of bikes rented on each hour of 27th of February 2011 then we are supposed to use data available to us prior to this date and not the whole of the data set. Pronojit,Benefit from segmentation is not expected. The contest rules state that one should comply with a the rule. It is not to make the contest easy but to make it more complex and thereby arguably interesting. Hi Gopi,Ideally how you can improve by doing either of the three things below-Varable Creation- Look at what other variables can be created which would help in increasing the predictive power.Dependent variable selection- Look at which is the appropiate dependent variable. you are predicting count. You can also predict registered and casual separately and add them to get your predictions. That should be better because both registered and casual have different patterns and therefore individual models for them should increase your prediction accuracy.Other techniques- One more thing you can do is to look at other techniques which might be usefull here. As it is partly a time series problem, can you combine RF with some time series elements. Or you might look at gbm. Or you can even try simple models like mean or medians. Sometimes simple models perform admirably and better than your complex ones. (PS- prediction = mean for (day_of_week, month and hour) - will get u a score of around .55)So try these things. Am sure you will improve your score by doing these.PS- Another thing i saw you using was as.factor(count). I dont think that is correct usage. you use as.factor for  categorical variables but count is a ration variable. I thought you could only use Random Forest to predict categorical variables (at least in R). Isn't that why you have to convert the dependent variable to a factor variable? Can Random Forest be used to predict continuous variables? Pronojit Saha wroteGopi wrotewti200 wroteFor example if we want to predict the count of bikes rented on each hour of 27th of February 2011 then we are supposed to use data available to us prior to this date and not the whole of the data set.Got it. Very interesting observations.  But not sure if we can train the model this way. I would be interested to know how we can train a model based on this approach.Very interesting observation indeed. Can everyone confirm whether this is actually what it means?So I did the segmentation for using data for training prior to the date that we are predicting for (like for predicting Jan 11 test data, use only Jan 11 train data, for predicting May 11 test data use only Jan 11 to May 11 train data, etc). After that I run a set of 24 random forest models for the 24 months predictions. But this did not improve my prediction as compared to running the same random forest model single time on the entire training set and predicting for the test set. As such I dont think there is any benefit of doing such segmentation. Would be good to have different views? Hello  , I  have  a question : why don't   you   use   temperature  as  a  predictor?Tnks Gopi wrotewti200 wroteFor example if we want to predict the count of bikes rented on each hour of 27th of February 2011 then we are supposed to use data available to us prior to this date and not the whole of the data set.Got it. Very interesting observations.  But not sure if we can train the model this way. I would be interested to know how we can train a model based on this approach.Very interesting observation indeed. Can everyone confirm whether this is actually what it means? Gautam Gogoi wroteVarable Creation- Look at what other variables can be created which would help in increasing the predictive power.Hi Gautam, maybe something other thoughts from you about variable creation, please. how can be decided what variables and operations to use. Also if you have some references about this subject would be greatThank you very much Gautam,Thank you for sharing your code. Nice approach with sqldf. Never thought I will use it :)I am trying to organise my code to set up model testing pipeline. The initiative helped to save time at the beginning, before I started exploring Random Forests. It takes from ten minutes to several hours to compute my current models (I use caret package to get unified interface for prediction) that slowed my iteration dramatically.It may happen that people somebody here will be generous to share best practices in organising code better to improve readability and efficiency. Please write me here or on github (may be a pull request?).I source the file below to get my training and cross-validation sets ready for model testing.https://github.com/artem-fedosov/bike-sharing-demand/blob/master/read_data.R Gautam,I tried a dirty version of mean prediction (the score of 0.55 amused me). However, I was able to get only 0.84 with the approach. May be I have interpreted / implemented proposed solution poorly?prediction = mean for (day_of_week, month and hour)Implementation:f <- function(x) {mean(subset(train, hour==x$hour, month=x$month, wday=x$wday)$count)}Prediction <- by(test, 1:nrow(test), f)Prediction[is.na(Prediction) | Prediction < 0] <- 0Prediction <- as.integer(Prediction)PS. I know that the rules demand using only data prior to prediction. Please look at it as on exercise to get better feel of data. wti200 wroteFor example if we want to predict the count of bikes rented on each hour of 27th of February 2011 then we are supposed to use data available to us prior to this date and not the whole of the data set.Got it. Very interesting observations.  But not sure if we can train the model this way. I would be interested to know how we can train a model based on this approach. \" You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\"If you are referring to this statement, it says about using all the training data for preparing the model. The training data contains rental information from 1st-19th of every month and we should predict demand for 20th given in test data. here is the link:https://www.kaggle.com/c/bike-sharing-demand/data My understanding is that we can make use of all the training data to prepare our model. Can you please share where you read that its not as per the rules? Gautam,Thanks for the tips and spending time to go through the code. I'll work on the suggestions and share how it goes. I'm surprised to hear that a simple mean would get  a score of 0.5!!",
    "Hi all,I just read the fine print in the rules:\"Your model should only use information which was available prior to the time for which it is forecasting.\"This seems to complicate things quite a bit, given that the forecast periods are distributed throughout the training period; for example, you can only train your model using 2011-01-01 -> 2011-01-19 for the forecast during 2011-01-20 -> 2011-01-31.This would (mostly) rule out using seasonal averages and the like and add the complexity of predicting ridership growth.I've just scrapped my model and started over. Anyone else overlook this? Doh, I have been breaking this rule too. Unfortunately, I was looking at some of the sample code other people have posted and since they didn't restrict their models to using only prior dates during training, I didn't either.To help other people not run into this same problem, I'm attaching a sample R program that follows this rule explicitly while training models. Hope it helps someone not make the same mistake I did.Cheers,Eric Dongming: to clarify, I don't know if anyone is using data other than what's provided. What Steven pointed out, and William confirmed, is a twist in the usual Kaggle rules. To be in compliance, you can't use TRAIN data from a time and date in advance of the predicted test value. So, for example, your first 1/2 month of predictions for the TEST data has to be entirely based ONLY on the 1st half month of TRAIN data. No training over the entire TRAIN data set and then predicting (except for the very last 1/2 month of TEST data to predict).More typically, and what I suspect many, including myself, had started doing, was using the entire training set to develop our models. Hi Haakon,I believe the intention of the rule was your 1st scenario. Personally I took the somewhat more liberal interpretation and used the entire day's weather data to make a prediction for any hour within that day (accidentally, until I noticed then justified it to myself). Itrainthe model using weather up to Jan 19 23:00, then to predict ridership for Jan 28 btw 6pm-7pm I use the weather for all of Jan 28 (that isI use 0pm-23:pm to predict 6pm - which is sort of cheating). I think of it as using a weather forecast for the day (which would be available and riders would be basing their decisions on it).  I also found it helpful to use the morning weather (btw 6am -10am) as a predictor for the rest of the day in order to capture some inaccuracy in the forecast.Regards,Steve Hi Eric,I just looked through theCorrectTrainingMethod.Rfile that you provided and I found itEXTREMELY USEFUL. Thank you so much for sharing!One thing that I might add: When I ran the code, I found that the YearMonth column in the test dataset erroneously contained three values for 2013-01 (when it should only go up to 2012-12). I think this is because I am in Pacific Time and deriving YearMonth values from timestamps that do not have timezone specified defaulted to my local timezone (Pacific Time).I was able to fix that issue by specifying the timezone in the as.POSIXct function:train$datetime <- as.POSIXct(train$datetime, tz='America/New_York')test$datetime <- as.POSIXct(test$datetime, tz='America/New_York')Instead oftrain$datetime <- as.POSIXct(train$datetime)test$datetime <- as.POSIXct(test$datetime)Thanks again!Best,Howard This is a murky rule. There's no meaningful distinction between a model, a parameter and a hyper-parameter.Selecting a model - even before it is trained -issaying something about the data. So on one side of the spectrum, you could build a model which hard-codes an entire prediction for the test set and prints it out. Clearly this one abuses the rule.But on the other side of the spectrum, there's a model which models the count as a binomial random variable rather than a Gaussian, and for all you know, the decision to pick this structure was based upon looking at the entire training set. Does this one violate the rule? Why?The problem is, all the models are going to exist on a continuum between these two points, and many will overfit by implicitely using future information.There is one way you can enforce this rule, using the minimum description length principle. Require models to take the form of executable code that sequentially outputs probability distribution for the count, and penalize them by 2^bit_length and a log scoring rule. It seems to me if you follow the rules, your predictions for the first months will be significantly worse than your predictions for later months.  I wonder if you can use this to catch \"cheaters\". I'm glad I read this thread. It renders this competition really uninteresting.I am also wondering if there are biases due to the date convention for testing and training. For example, it would be good to look at the time of the month in the D.C. area when welfare and assistance programs pay out monthly checks, and also whether many employers pay bi-weekly, or just once per month, in the area. I'm not sure if people will be more likely to ride when they just received a pay check, or conversely more likely to ride when they are waiting those final few days before the next paycheck, or perhaps there is no effect? The data doesn't enable me to see.Another effect could be winter-time holidays, which occur across longer durations (people usually take more time off for Thanksgiving and Christmas than for 4th of July or New Year's) but are going to be in only the testing set. The way training and testing is segmented in this data set is frustrating for this sort of reason. We can't really suss out if there's just something different about choosing to ride a bike in the first 2/3 of the month vs. the final 1/3 of the month. I've a strong suspicion this throws out 90% of submissions so far (including mine)!  I know I missed that. When I tried predicting from previous date-times I only used previous values, but the parameters for averages and other factors were calculated over the whole training data set.I could see a real-world case where you obviously only have the initial data for the 1st prediction, and the model could improve over time, but in that case you wouldn't have the large gaps in \"train\" data for the later periods. Month 13 predictions would have 12 months of actual data, not 1/2 the data from the previous 12 months, etc.Not sure if I'll figure out a new approach or just use this as a learning exercise without this twist.  Unless everyone restarts and follows this rule, it will be hard to assess performance vs. what others are achieving. Thanks for calling this out. I know it is a bit nontraditional, but I wanted to throw in a twist on the usual \"train on year A, test on year B\" approach. One of the downsides of competitions is certainly our inability to enforce the \"soft rules\" that one would follow in real-world modeling. I think that's a really excellent question, something I've wondered about too. I don't see a way to enforce the rule, short of submitting a model or a program (which would then be executed by Kaggle) instead of submitting a list of data. Does anyone else have an idea about how to enforce the rule? It seems like it would be important to figure it out, since time series problems are very common and useful. Oh wow. I need to re-do everything.... :'(Thanks for pointing it out, Steven. thanks for the CorrectTrainingMethod.R  R example ! There's no way to reboot this competition? I looks like 90% or 95% of the people are not following the rules...so how can i compare my performance?There's no way to re-format the train/test datasets, in a way which makes it easier to follow the rules? To me it seems as if the train/test split intends to approximate an online-learning problem where a model updates incrementally in response to new ground truth and perhaps weights more recent examples more heavily than less recent ones.Unfortunately without giving the correct labels for the test set the problem doesn't really require the use of online learning.This is a good rule - it really makes the solution much more interesting than simple train/test batch mode learning.  The time series data makes for some interesting features. Hey guys... New to Kaggle and happy to be here.Does anyone have insights into the reasoning behind the training/testing set splits.I'll admit that I didn't read carefully enough before jumping in and did not notice the rule in discussion until after going back and re-reading. I only went back to re-read because I noticed missing dates... then I noticed that I was missing the same ones each month. HAH, go figure. I assumed that sort of regularity couldn't have been on accident.Anyways, it just seems pretty impractical to be modeling like this. Unless I'm missing a bigger picture here, it does not resemble any real life application. My assumption on the purpose was to be able to learn from the data that we have so that the \"model\" could be used to predict future volume based on criteria... NOT filling in the blanks from missing date.Even interpreting the rules as \"you can use all previous data (not just current month)\" doesn't make any sense either. You're either still missing information or you're modeling off of predicted values which I wouldn't image you would do in the real world either. You'd based future models off updated real data not the data previously predicted, right?Like many other folks... I'm completely uninterested in spending more thought against this particular competition. That being said, I'm wanted to ask if there was a bigger picture reason that I'm not getting. I have another rules-related question - can I use more data? Website of original data provider has much detailed info - about duration of every trip and start/endpoints, for example.If I cut from whole dataset records for datetimes into train set - it's ok, since I'm not using any information from test set.Right? @mr_englishI have a score of 0.486 on the leader board using random forests. I wasn't following the rules, however, and used the entire training set for predicting the count. I'd venture to say that probably 90% of the people with similar scores (0.48-0.51) went the same route. It'd be nice to know if anyone in the 0.30's or low 0.40's was able to do it while obeying the rules. A 0.54 seems pretty good for following the rules. I am wondering how many submissions could be flawed. A lot of people on forum talk about cv scores, which makes me think that they are using the entire training set.I was able to get ~0.54 on the lb by using two simple models, one for each target, based only on previous dates. I would like to know what other people with better performances are doing. Though this problem is strongly related to date and time, it does not look like time-series problem to me. My experience is that I took each hour as an independent observation. I feel that there might be some continuity information between hours lost which could be useful for prediction. Back to this thread, if each hour is regarded as independent, it may be \"OK\" to use data from next month and after, though that does not justify us in reality. So I did the segmentation for using data for training prior to the date that we are predicting for (like for predicting Jan 11, 2011 test data, use only Jan 11, 2011 train data, for predicting May 11,2011 test data use only Jan 11,2011 to May 11,2011 train data, etc). After that I run a set of 24 random forest models for the 24 months predictions. But this did not improve my prediction as compared to running the same random forest model single time on the entire training set and predicting for the test set. But probably, this is the right way to go forward. Thanks for a very helpful and quick reply!! I will continue following my 1st interpretation.Regarding your other comments, it is interesting (and makes sense) that the morning weather is helpful. I would also believe that the weather 1-2 hours prior to the forecast period is better than the contemporaneous weather. (Based on personal experience: when I used a similar bike sharing system in Oslo, I would typically make the decision on whether to travel by bike or bus at least an hour before I actually \"checked out\" the bike). Hi,I am new to data science and this is my first competition, so I have some very basic questions regarding the rules. In particular I am uncertain of how to interpret the rule mentioned in this thread: \"Your model should only use information which was available prior to the time for which it is forecasting.\"I suspect I over-complicate things here, but I struggle to decide which of my three interpretations below is correct? From the above discussion I understand that parameters of the model must be based on the training sample for the first two weeks of the respective month and prior months. My question is regarding access to data on feature variables when using the estimated model to make predictions:1. Only use information from the first two weeks of the month (and prior months) when estimating the parameters of your model. Using these parameters, predict bike demand for each hour of the second half of the month,giventhe data on weather, humidity, etc for that respective hour. Eg: when forecasting demand for 28 January between 6pm and 7pm, I can use data on weather, humidity, etc, up until 7pm that same day.2. Only use information from the first two weeks of the month (and prior months) when estimating the parameters of your model. Using these parameters, predict bike demand for each hour of the second half of the month given the data on weather, humidity, etc for all timesup until the hour prior to the hour for which I forecast.Ie: when forecasting demand for 28 January between 6pm and 7pm, I can use data on weather up until 6pm that day.3. Only use information from the first two weeks of the month (and prior months) when estimating the parameters of your model and when making predictions. Ie: when forecasting demand for 28 January between 6pm and 7pm, I can only use data on weather etc up until 19 January 12pm.In the third case, most explanatory variables become quite useless as bike forecast will depend on a highly inaccurate forecast of the weather. As such, the only useful explanatory variables will be the ones that are known, eg day of the week, time of day, working or holiday, etc.Thanks for all help! I'm actually finding that rider growth isn't such a big deal, since you are only making projections ~2 weeks into the future (however you do it 24 times). Using the average from the 1st half of the month that your making your prediction for gets on the right scale. ##EDIT##ViennaMike - ok, makes sense now - now just have to figure out a way to factor in the rider growth...that's the tricky bit###ViennaMike - if I understand correctly, you're saying that :2nd half of month1 is predicted by subset(1st half of month1)2nd half of month2 is predicted by subset(1st half of month1 + 1st half of month2) etc..etc..The way I understood the rules is2nd half of month1 is predicted by subset(1st half of month1)2nd half of month2 is predicted by subset(1st half of month2) etc..etc..so the training set in each iteration is only as big as the number of row in each month/year combo.I'd be interested to hear how others interpreted it. I've done up a CART-model for each, but haven't submitted yet, just finetuning the code. Since I don't realize that the whole data set is actually available online, my submission(so far) is absolutely based on the train data set only.",
    "I've run into local cv-score/public score mismatch (huge mismatch, actually). Both used to be pretty much the same, but after some changes (in model hyper-params) this behavior changed - local cv rmlse score got a little better and public score dramatically moved up (from 0.4 to 1.1). And I can't find good explanation of this.Here several options I see:1. The changes played good with local train data, but [very] badly - at public test data. It in fact can be a coincidence - model with new params plays well on one bunch of data and bad - on another bunch of data. That means... what? With assumption, that models plays good only on train data, there is nothing I can do to check this - all tests will be good (since I have only good bunch). This option seems unbelievable to me. Maybe there is theoretical way to prove, that this is impossible? Or this is possible and has some name (and ways to fight with it)?2. I failed somewhere. I do it all the time, so this is most possible option. But since model worked fine until last commit and last commit has only couple of lines with constants changes, I assume, there is something wrong with model validation. Here is how I calculate total score: I use KFold CV, compute rmlse for each, then compute rmlse between vector or cv scores (from each of k-folds) and zero vector. I vote, that the algorithm has mistake somewhere, but why it worked fine before? If its wrong - how total score should be calculated?I understand, that my questions are rather basic, but learning by doing seems best way for me here. I'll be glad to see references to books/articles with the problem's explanation.Thanks. Two words: \"cv set\".",
    "hi all,from my limited knowledge, I used statistical regression to find the true false value. can anyone here provide the confusion matrix as to which %population we be the target group?",
    "I've been trying to binarize some of the contest's data fields and a couple of questions came up.1. Why feature binarization should work at all? Let's use 'weather' column as example: yes, weather = 4 is not twice cooler (or something) as weather = 2, but after binarization I have four columns (let's say 'weather1', 'weather2' and so on) that are not independent. For example, if 'weather3'=1 for some row, then 'weather1', 'weather2' and 'weather4' are 0 for the row. So I now have a lot of new features that gives me no use. Why should the approach work at all?2. What fields are worth binarization? 'weather' is good example, but same logic can be applied to almost all integer columns - for example, 'date day' or 'date month' (and thats a LOT of new features). So when trying this I used PCA to decrease dimension of feature space. And it gave me no use - I couldn't get results better that result I get without binarization+PCA. That leads me to bonus question: how can I find out if this preprocessing technique worth using before actually trying it.I would be glad to get any help - please, explain or give referense to good data source or tell me where I'm wrong. I used feature binarization with Neural Networks (in SPSS Modeler) and this brought a great improvement (even though a NN can approximate a non-linear function) .. so my rule of thumb would be to try it for everything but trees.And since you asked: I simply used it on everything where it seemed sensible: year, month, day of the week, hour of the day, season and weather .. and yes, this are a lot of new features... but who cares? You can select the best features later - if you like. Uh, Triskelion answered me, I'm so excited.)) Your blog is so helpful for a newbie like me, thanks a lot. Actually, \"helpful\" is wrong word - it has a tons of useful info, it is strongly related to what's going on, it is very motivating... I just want to say thanks.Back to feature binarization... Isn't it to general to say, that binarization worth trying in all linear models and forest-like models with not deep trees? I'm trying to get kind of rule of thumb. 1. It depends on your algorithm/model. For instance with random forests you can keep a categorical column to a 1-dimensional vector. Even though 'weather=4' is not twice as cool as 'weather=2', random forests will still work. SeeKaggle Titanic Random Forests Tutorial:Although they are strings, the categorical variables like male and female can be converted to 1 and 0, and the port of embarkment, which has three categories, can be converted to a 0, 1 or 2 (Cherbourg, Southamption and Queenstown). This may seem like a non-sensical way of classifying, since Queenstown is not twice the value of Southampton-- but random forests are somewhat robust when the number of different attributes are not too numerous.Other algorithms like logistic regression may have a harder time when you count-encode instead ofone-hot-encode(see link for explanation on why individual weights may outperform shared weights).This is not true for all learning algorithms; decision trees and derived models such as random forests, if deep enough, can handle categorical variables without one-hot encoding.2. You can find out if a preprocessing technique works, by adding it to the pipeline and checking if the local cross-validation scores improve or degrade.",
    "Lets look at relation of bike sharing count from windspeed not ad working days (it's uploaded below).Weird results, isn't it?Anyway, I want use the information in my algorithm. So I add feature that takes 1 when there is not working day and speed in range [1, 5], 0 otherwise. And score goes up, a little bit.My question is: is it ok to create such features? I can't guarantee, that observer relation will be kept in all data, so there is chance that algorithm will perform better at existing data, but actual performance can be same (or even go down).Thanks.",
    "Hi, is there a way to opt out of this competition. I got started a few months ago, but dropped it after seeing the how some contestants had misunderstood the contest rules. I basically had no interest after learning about this, and I'd like to drop this from my \"active\" competitions.Thanks! There is no way to delete your submissions, but if you stop participating it will eventually drop out of your list of active competitions.",
    "One of the rules says \"Your model should only use information which was available prior to the time for which it is forecasting.\"I see few potential interpretations:In my model equation I can't used data from future (for example variable average temperature tomorrow is forbiden)When I train data for given month I can't have observations from future months in my training data.It is forbiden to have time variables that also takes into account observations from future.In other words is model with one categorical variable year build on whole training data set legal?",
    "Scoring system is a little weird. Below is two data points (fictional) out of 6493 (test set). You can see that you’re getting punished much more severely for the first data point, even though first data point is off by 14 while second data point is off by 500. Keep in mind that your score is simply square root of an average of all errors. Am I missing something?Actual        Predicted       log(Actual+1)     log(Predicted+1)           Error1                15                  0.3010               1.2041                           0.8156500            1000              2.6998               3.0004                           0.0904Where error is simply (log(Predicted+11) - log(Actual+1))^2 Think about it proportionally. In the first case, your estimate was 15X off. In the second, it is only 2X off. If you did not have the log and instead used some form of an absolute difference, you would have a situation where predicting 9990 when the actual was 10000 is worse than predicting 2 when the actual is 10. The natural noise/variance around 10000 is going to be larger (on an absolute basis), so predicting well within the standard deviation is to be rewarded.Another analogy is to ask which you would rate as more impressive: guessing there is 1 M&M in a jar of 15, or guessing 500 in a jar of 1000? Understand, it would be a case if we were predicting stock price, where proportion is all that matters (mostly). But, in this case it seems a bit off. If I were to run bike renting company I would rather have a model that predicts 550 when actual is 500 and 15 when actual is 1. Ruther then a model that predicts 1000 when actual is 500 and 2 when actual is 1. But, I could be wrong in my reasoning.There are just too many unknowns to be so precise when it comes to very small \"counts\". Anyway, this competition is fun and some good practice. Sorry I used log instead of natural log (ln), anyway, point is the same:Actual      Predicted         ln(Actual+1)          ln(Predicted+1)        Error1              15                    0.6931                   2.7726                  4.3241500          1000                6.2166                   6.9088                  0.4791",
    "Hi All,I have spent significant time on re-engineering the features. These are the subsets which I have created:- Working days (days which 'holiday' = 0 and 'workingday' = 1)- Weekends/Holiday days  (days which 'holiday' = 1 or 'workingday' = 0)And apply the same logic for casual users...Here is my question:Can I train a model foreachsubset (for working days and non-working days)? Then using the test set, break the data down into each subset and pass the relevant data through each model to obtain a count.In the attached files are two graphs showing show the registered users have different patterns for either working or non-working days.Many thanks in advance,Anton Hi Anton -I don't see any reason you can't take that approach.  It sounds like a logical next-step from creating two separate models for \"casual\" and \"registered\".  It will just require a little extra coding when generating the predictions from the test set.  But since you already created the subsets of the training data, it shouldn't be too difficult to apply the same steps to the test set prior to generating your predictions.",
    "Hey,For some reason there's a really big difference between my cross evaluation score and the public leaderboard score. Is this normal?I did an 85/15 train/validate split, I score:.19 on the train model (due to overfitting).31 on the validation set.43 on the public leaderboardTried different seeds as well all with similar results.",
    "that the score can be almost zero???!!! that two submissions disappear...I got \"Your submission scoredNULL, which is not an improvement of your best score. Keep trying! \"Any one knows why? From the rules:Due to the public nature of the data, this competition does not count towards Kaggle ranking points.We ask that you respect the spirit of the competition and do not cheat. You should not submit entries based on test-set answers or train your model on the test set. Hand labeling is also forbidden.",
    "Hi All,As part of an academic exercise, I created a data set profile for the training data set associated with this competition and wanted to sharing it with everyone. Hopefully it will be useful. I've attached it to this post, but it can also be found through the following link:http://bit.ly/Zjx1vuBest of luck to you all!Daniel Great stuff ... really useful.Also just wanted to point out that holidays != weekend -table() command on a part of the training setFri Mon Sat Sun Thu Tue Wed40 177    0     0     0     0    17",
    "In the data description 'atemp' is described as \"feels like\" temperature. What does that mean? If it really is \"feels like\" temperature then who decides what does it \"feel like\" ? Why is there so much of difference between 'atemp' and 'temp' ? \"Feels like\" includes the impact of things like humidity and windspeed on how hot or cold it \"feels like\" to a person.  As an example, the measured temperature may be 90 degrees F outside, but if there's a very strong wind, it will \"feel like\" 80 degrees F.  It's more commonly reported as the \"wind chill\" or \"heat index\" depending on the situation.",
    "I just found out that weather ==4 only has one data in train.csv (index 5632) which makes weather4 has no predication power. I really doubt about the reality of the data. At least the month and season are consistent.And the season and temps are consistent if you label them yourself. :)Season 1==Jan to Mar and temps are coldest at .82-29.52C aka \"winter\". 2==\"spring\" Apr-Jun with temps 9.84-38.54. 3==summer Jul-Nov and temps range from 15.58-41C; and 4==fall Oct-Dec with temps 5.74-30.34 similar to spring again.",
    "Hey,I'm new to Kaggle. I just wonder how to know if my model works on the test set? since the real counts are not provided from test.csv.Thanks Hello Mayur,I didn't use the rmlse package/function but wrote my own version (hope it's correct, but it appears to return useful numbers so far). In this example I used a lineair predictor:a=validSet$countp<-predict(glmModel, newdata=validSet, type=\"response\")n=nrow(validSet)validRMSLE=sqrt(sum((log(p+1)-log(a+1))^2)/n) Great, thanks Frank!! Would you also know how the rmsle() command is to be applied in R to check how much error exists in the model? Mayur,This is my code to split the full training set into a train and validation set (i chose here to use 10% for vaildation):# split train and validation set##set.seed(12345)load(file=\"./data/enhanced/trainFull.Rdata\")validset=sample(1:nrow(train),nrow(train)/10)valid<-train[validset,]train<-train[-validset,]save(train,file=\"./data/enhanced/train.Rdata\")save(valid,file=\"./data/enhanced/valid.Rdata\") Hi,Could someone please share a generalized R code for splitting a kaggle \"train.csv\" file into its own test file, and then use \"rmsle\" command to compute the error? Thx",
    "Hi guys,Using ShuffleSplit I get constantly underestimated CV scores: 0.41 +/- 0.1 while the public score is about 0.48. This is with 90% train, 10% test split.Using 40% train, 60% test my CV scores goes up to 0.44, still underestimated.I even tried splitting train/test data according to the day number, so the same split the contest uses, and the CV score still is about 0.44-45 for a submission that gets 0.48-0.49Do you have any idea why this could happen? Is it normal or too much? I'm a newbie and would like to understand.Thanks! I'm using my own ham-fisted regression with some tweaks and it consistently CV's at ~.30 but PL's at .40.This is pretty typical of all kaggle comps. At least for me. :)In this case that day split may also be having something to do with a discrepancy. Will monthly pay day have some influence in the target country? Particularly, this is a situation that is leaving me baffled:A simple DecisionTree with hour, weekday and year as added features gives me 0.443 as CV score (using day based split) and 4.75 as PL score.I then tried to replace the year feature with \"month number\" e.g. February 2011 = 2, March 2012 = 15.This improved my CV score to about 0.424 but it worsened the PL score to 0.516.I don't see why the CV score would be better if the model is actually worse, especially considering the month number values are the same in the train dataset and test dataset.",
    "I split my training set as 80% train and 20% validationI tried negative binomial after delete some insignificant variables(month, day, etc) however, prediction performance was worse than ln(y+1) linear regression.Can any one explain it? y|x fits negative binomial for me.",
    "I used rfcv (randomForest) to estimate how many variables to use to minimize the error or MSE. I got the result:> rf$error.cv12             6             3             14544.523 4369.261 12429.269 15922.400but based on the examples I found online, the error rate was between 0 and 1. Why did I get such large error rates for five-fold CV ?",
    "Just wanted to share a simple solution (Error of 0.69344) using Excel...I wanted to explore the issue where we aren't supposed to use any forward looking training data.  So for January 20-31st  2011 you can only use January 1-19th 2011 training data.I figured if I only ever looked training data for the current month, I would always adhere to the backward looking rule.When I graphed the demand I noticed visually that the biggest variations were time and workday.  So I did a vlookup using month, year, time, and workday flag.Obviously this isn't a *great* solution, but it was interesting to get useful data from it.-Jeff",
    "Hello Everyone,Can someone please tell me why Casual and Registered variables are not present in the test data set?I basically have to set these two variables as null in the training set and then build the model so that the predict function could work. Hi Syed,if I understand it correctly there are two types of people that can rent a bike. The first type of people are the \"casual\" ones, meaning that they haven't signed up for anything and just use it for one afternoon or so. The other type of people are the \"registered\" ones. Those have signed up - probably to save money - and probably use it regularly.If you now want to have the total amount of people renting a bike (= \"count\") you just have to sum up the casual rents and the registered rents.Since Kaggle gives you the the number of casual rents and the number of registered rents I see two ways to do this problem:1) Build a model to predict the casual rents and a model to predict the registered ones and then add them up to get the total rents2) Build just one model that directly estimates the total rents Ankush -- of course he can't.  Think about what you're asking here. i get it,i mean i can't compute the test err of my algorithm while i don't know the labels of test data.But we can use cross validation to compute the test error,not by using the data in test file,I misunderstood it before. bdc wroteIf test set had \"count\" or \"casual\" and \"registered\" what would you be calculating? If test set had \"count\" or \"casual\" and \"registered\" what would you be calculating? excuse me,i don't understand why there isn't \"count\" in the test data,could someone tell me why we don't have \"count\" in the test dataset Just to add to the solution, understand that casual and registered are labels and not features. Sum of casual and registered will give you count. I hope this helps ClayDogg wroteSince Kaggle gives you the the number of casual rents and the number of registered rents I see two ways to do this problem:1) Build a model to predict the casual rents and a model to predict the registered ones and then add them up to get the total rents2) Build just one model that directly estimates the total rentsHi, could you comment on which method out of the two mentioned would be better and why?",
    "Hello Everyonehow does re factoring affect model performance.e.g. there are 10 levels and i reduce it to 5 or something. Does it affect my algorithm outcome.  ???Please respond if some one gained performance like this.Note : I am not talking about change in numeric to factor variable .To me it doesnt make any difference",
    "I wrote a blog post about this problem way back in March. Not saying these folks stole the ideaaaa but...enjoy :)http://abe.is/analyzing-citibike-usage/",
    "Hi,I've created a new variable on the training set that represents the hour of the day for each row.I've tried to apply the same code (in R) to the test data set but it won't work. Has anyone else had a similar experience? I had the same issue. I think excel reformatted the datetime variable. The solution linked below helped me out.https://www.kaggle.com/c/bike-sharing-demand/forums/t/8343/i-am-getting-an-error/47402#post47402 No, I think most of us who did this didn't have any problems. If you'd like to post your R code I'm sure we can get to the bottom of it.",
    "Has anyone tried this approach by predicting casual & registered separately & add them to get final figure.. i was wondering it could give better counts.Please share your views on this The mean of percentage of registered out of total count is about 85.53%.So if you did predict them separately, you could take the weighted average of the values to predict the final count. I predicted the registered and casual separately upon my partner's suggestion. It does make a difference. I don't think you'd want to do that - if you predict them well separately, registered out of total will come out around 85% without having to weight. Hi parndsheel, I am trying to model them separately and then add the results later on. What I am observing right now is that in my overall score, the predictions made from registered has a lot greater impact compared to the predictions from casual.",
    "I'm surprised nobody has commented about this yet, but I see that the Leader has an error metric of 0.0000.  I know this is mathematically possible, but the only way I can think of getting that result is that someone had the actual test data.  Any other possibilities? I guess I don't understand how they cheated - they must have found the actual data somewhere?  Absent that, I can think of many ways to cheat, but none that would give you an error of 0. People cheat (mostly themselves) on competitions with public ground truth (like this one). They receive no points, no fame, and--depending on our mood--an account block for doing so. Dale, the dataset for this competition is public, and even linked on the home page of this competition. That's it.",
    "Hi all,I found there were about 1000 zero-windspeed observations in train dataset. To me there are three explanations:1. windspeed is zero indeed at these hours.2. windspeed is too low to be measured, for example varying from 0 to 5.3. these zeros or part of them are nothing but NAs.Do you think which one is more likely to be true? Thanks. FWIW, I did submit my latest model without windspeed.. I just removed it entirely. Model improved by 0.00005.. so not much impact there, at least on my end. At this point I don't think it's worth digging into. John, I plotted train + test dataset. The windspeed data in range between 0 and 6 are still missing. Unless there is a different way to find out, I tend to agree with the second explanation. In this case, I would not expect windspeed below 10 or 20 affects bike rental much like Brandon said. For now, I just leave it as is. I didn't go as in-depth as you in looking at it, but I agree. Even from just a common sense perspective, if I were renting a bike, unless we're talking monsoon winds out there, I'm really not going to care what the windspeed is. It just wouldn't factor into my decision to rent a bike. I haven't yet dropped it or smoothed it out yet for my model, but that's one of my next steps. Yunfeng,Imade a quick histogramof the wind speed entries; I also looked at the raw numbers themselves.1. As you say, there are no windspeed values between 0 and ~6. I agree with your option 2, that anything less than 6 is just too low to be measured.2. The wind speeds are reported with four decimal places of precision. However, all values are within less than 1% of an integer. That is, you see a bunch of 15.0013, but absolutely no 15.3426. What's more, the decimal part for a given integer seems to always be the same: if you see 15.0013 you'll never see 15.0012. So I would ignore the decimal places and assume the wind speeds are only accurate to the nearest integer.3. But it's slightly worse than that. There are a couple of wind speeds that are completely missing. There are no entries for 10, for example. (It alsolookslike there are no entries for 9, but this is just because there are a bunch for 8.9981 that get put in the lower bin. Maybe I should have set the bin widths to 0.99 instead of 1 so this artificial aliasing wouldn't happen.)Overall, I would say the wind speed data is highly suspect. Tread with caution. (Also note that I didn't look at the test set. Maybe it's different.)",
    "Guessing the hour is an important variable I thought I could easily convert the datetime after reading df with pandas something like this:df['hour'] = pd.to_datetime(df.datetime).hourbut no that doesn't work throwing an error like this:AttributeError: 'Series' object has no attribute 'hour'because df.datetime isn't a single value even though some of the example code looks like it is e.g. things like df['new_col'] = df.col1 + df.col2In the end I realized I have to map over the column like so:df['hour'] = df.datetime.map( lambda x: pd.to_datetime(x).hour )You can also have Pandas parse the column as a datetime in the first place like so:df = pd.read_csv('train.csv', header=0, parse_dates=[0])and then the lambda becomesdf['hour'] = df.datetime.map( lambda x: x.hour )If anyone has an even better way please let me know I propose an other way based on the Hussain's function.import pandas as pddata = pd.read_csv(\"train.csv\", sep=\",\")def splitDatetime(data) :datatime = pd.DatetimeIndex(data.datetime)data['year'] = datatime.yeardata['month'] = datatime.monthdata['day'] = datatime.daydata['hour'] = datatime.hourreturn dataI guess it should be slightly faster I did it like this:train = pd.read_csv(\"train.csv\")train[\"hour\"] = [t.hour for t in pd.DatetimeIndex(train.datetime)] In   R:data is  the  var  that  stores the test  or   train setJust   write :data$datetime=as.POSIXlt(data$datetime)$hour I read the entire csv into a pandas dataframe, using the datetime column as a DateTimeIndex:train_dataset = pd.read_csv('data/train.csv', index_col=0, parse_dates=True)From there it's incredibly easy to add feature columns for datetime attributes:X = train_dataset.iloc[:,:-3] # last 3 columns are y, not XX['weekday'] = X.index.weekdayX['hour'] = X.index.hourX['year'] = X.index.year Thanks - I'd seen that post but all that string wrangling makes me sad when there is a library function for datetime to do it.However that's a good tip on making a custom data from some data - as a complete noob to Python, Pandas etc. there's a ton of tricks to pick up.  And yes, putting the munging code in a function to DRY it up for reuse on the test data is great practice (I noticed none of the tutorial code did that - lots of duplication). Use following function, already provided by one of the kaggle fellow in some other thread.Note: Don't forget to apply this function on both training and test data.def splitDatetime(data):sub = pd.DataFrame(data.datetime.str.split(' ').tolist(), columns = \"date time\".split())date = pd.DataFrame(sub.date.str.split('-').tolist(), columns=\"year month day\".split())time = pd.DataFrame(sub.time.str.split(':').tolist(), columns = \"hour minute second\".split())data['year'] = date['year']data['month'] = date['month']data['day'] = date['day']data['hour'] = time['hour'].astype(int)return data In RI extracted hour like thistrain <- read.csv(\"train.csv\", header=TRUE, stringsAsFactors=FALSE)library(\"lubridate\")train$dt <- ymd_hms(train$datetime)train$hour <-hour(train$dt)",
    "I've already made a submission with Python and I wanted to have a go with R. However, I'm having two issues:1.) as you can see from the picture attached my linear regression seems to be predicting minus values for my count!2.) Also, it's giving me a range of scores rather than a single value as my prediction.Can anybody guide me as to where I am going wrong on both counts? Another option is to regress on ln(Y+1). The transform of this variable would bound all predictions from 0 to infinity exclusive. 1) try poisson regression with a glm call and parameter family='poisson'2) you're predicting the mean and a confidence interval (upper + lower bounds), so you get three values It is not possible to know what your submission will be exactly as you do not have the actual counts. However, you may split your training set to train and cross_validation sets. Then train your model on new train set and calculate your rmsle on cross_validation. It should give you an estimation of what your rmsle will be.You may easily split your training set into two (here is a full example on github):https://github.com/artem-fedosov/bike-sharing-demand/blob/master/read_data.R#L36The metric for the competition is rmsle.You may either implement it as proposed by Tyler here:https://www.kaggle.com/c/bike-sharing-demand/forums/t/9941/my-approach-a-better-way-to-benchmark-pleaseOr use R's Metrics package to calculate it:library(Metrics)rmsle(actual, predicted)PS Sorry, WYSIWYG breaks code formatting, so I keep only links so you will be able to get to correct code. I checked the mean and the variance of counts they are not equal. the variance is way larger than the mean. I don't think the poisson regression would be a good choice Using ln(y+1)  as dependent variable is a much better solution than using Poisson regression, which needed more unrealistic assumption. It's totally possible to get rmsle under 0.5 by using just linear regression. Hi, is there anyone who used Negative Binomial regression in R  (nb.glm from MASS package)?I used it and ended up with  0.71478, which is not a good model. I have doubt on my code.Mahasen HI Artyom, how we calculate rmse before submitting? I am wondering how you are able to reach rmsle=0.5 with simple linear regression (lm). I suppose you use same features as I do: all we had plus all you may extract from datetime field.Could you give a hint to me: what gave you best results - model tuning or feature engineering?Travis, thank you for telling about Poisson regression. It gave me 0.68 when I was able to reach only 0.84 with default liner regression (lm) settings. Thanks Travis. I'm down to .5 on the leaderboard now :-)  I haven't looked at poisson regression yet, but I will.",
    "I got the following feature importances with the Random Forest:hour 0.6084year 0.1877temp 0.1253atemp 0.0407month 0.0234workingday 0.0097season 0.0023weather 0.0013humidity 0.001day 0.0001windspeed 0.0001holiday 0.0Kind of strange and makes me wonder a bit about the training data quality... If you are using random forests algorithm in R, like Rpart, there are ways of getting the importances learned by the system as one of the defined functions. How are you using CV on the previous data? I am doing prediction on the last days of each months using only previous dates. Should this be required by the rules? How do I actually interpret the result and move on from that? I understand that it is giving me the split for the decision tree but if I delete the feature with the least information gain, my result does not actually get better although the feature im removing is just like 2 % based on my decision tree another question is, should I do some feature extraction on the feature with the higher information gain?These are my result for registered users:0.247832329043 pHour0.0410931039364 season0.0531896878482 workingday0.053362198548 weather0.210081102929 atemp0.0237012346858 pWindRange0.0420803539752 pHourRangeRegistered0.21417684757 humidity0.114483141465 pMonthI trained casual and registered separately because looking at the hour range (im using R), the distribution is totally different. But the result is not improving much. Soumyajit: some of the most important features are \"hidden\" inside the datetime first column, so it's essential to extract them from it. Hour in particular, my simple estimator goes from a CV score of 1.30 to 0.48 just by adding the datetime hour as a feature.Another qualitative indicator of the importance of hour: Hey you have a pretty decent rank, you did not manipulate anything from original data?? Did you extract date, month, year, time from first column?In sklearn you can get feature importance by:model = est.fit(X, Y)print model .feature_importances_ Could I know  how you  manipulate the  origin data?I don't know how to  improve my score? Thank you for your reply! I want to know how you guys get the feature importance? Is scikit has the api? Strange. I tried different options on RF and also ET and AB, but workingday is still not important.It's like we are using different data sets. Is that possible? I just downloaded yesterday. To improve my score I actually made some data manipulation. But even with the initial dataset without any modification I got a score around 0.44 with the following variable importance:Hour : 52.4 %workingday : 14.7 %Year : 10.3 %temp : 3.8 %atemp : 3.3 %weather : 2.9 %humidity : 2.7 %season : 1.9 %windspeed : 1.3 %holiday : 0.6 % I tried also with up to 300 trees, but the result is similar.Were the main feature importances the same in your result? e.g. hour, year, temp?Did you do any algorithm tuning, data manipulation or feature engineering? Your result is very good, so I presume you did so ... I did not use scikit-learn but 10 trees seems quite low. Nevertheless, this cannot explain why workingday has so little importance ... Hi Toulouse,Strange. I just run the random forest regressor in scikit-learn with the default parameters (n_est=10) and using the original data.Did you change anything? Hi!The importance of workingday seems very low. With RF, I got around 0,14 ...",
    "I saw (on github) that some people using cheating techniques. They found full dataset from the internet where there missing values from test set. So they training their models on a full dataset and got pretty good scoring results.What score is considered very good and can be reached without cheating?",
    "As I understand the data used for this competition are the same as in https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset which has the following comments:The core data set is related tothe two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USAWeather information are extracted from http://www.freemeteo.com.I would like to use more detailed whether information for the model(i.e. split \"weather\" column to Visibility and Precipitation, so I tried to find the corresponding data at freemeteo.com, for example here are the observations for 1/1/2011http://freemeteo.ru/weather/washington/history/daily-history/?gid=4140963&station=19064&date=2011-01-01&language=english&country=us-united-statesand the values are not exactly the same as in this dataset, f.e.:temperature in freemeteo is 2 degrees of C, but temperature in the dataset is 9.84 degrees of C and so on.I guess that the dataset's weather were obtained for the location other than Washington, D.C., but have no idea about what it would be. Google does not help.Does anyone have any related information?",
    "Hi all!!Happy to be part of this community with my first data challenge here :) (excluding the Titanic challenge). I just started exploring the data in Excel, but noticed already some gap in my knowledge to tackle this problem, so hoping to find some guidance.I just wanted to start with a simplified approach, checking the relationship with time-of-day and bike rentals visually, also revealing the interaction between time-of-day with weekdays (see attached file, haven't imputed data for time gap, but on a conceptual level not of importance to this question for now).How do you model such a non-linearity?The pattern seems very consistent and of importance to predict the biker count for the test data. I have searched google for some examples of time series analyses but haven't been successful in figuring things out. Have you usedCubic Relationship, or some other kind of polynomial function? Or do you split the data and model the subparts. Can someone point me to resources to tackle this problem?Thanks a lot!!Correction: sorry accidentally updated two files, second chart file will do. First chart file also consists of relationship between time-of-day and bikers for the different seasons.Update:so I just made my first submission :) haven't done anything fancy, but calculated averages by time of day, controlling for weekday. Benchmark score: 0.69310. Hi thanks a lot!I did it that way in the end. Just interested to see how one would approach such a question when you would consider the time data a continuous variable, but maybe a bit of a stupid question since it's irrelevant to this problem set.I'll just proceed this way and add new stuff :) Hopefully a time series problem will occur in a future challenge.Sarah Dummy variable works here. In a nut shell, to make your prediction based on a conditional mean i.e: E(count | time == 14:00, workday == 1). Yeah, sorry didn't mean any confusion there. Just didn't correct it, since this wasn't relevant to the question, just served as a mere support of non-linear trend that I would like to predict if you see past the gap.Must have went something wrong with conversion or something like that. Below should be a more correct depiction. But so no implementation of a prediction based on a function that estimates points in a non-linear way? Is there something wrong with your data?None of the data point with time tag \"14:00\" has zero count, but your plot looks like all the 14:00 points are valued zero or missed? This is just part of the approach to the whole prediction. There lies a lot of information in these variables, so I want to approach the problem in this simplified way and learn how to deal with time series such as these. I don't want to just put a whole bunch of variables in the analysis.Thus my question doesn't pertain to the whole \"best\" solution with all possible variables, but just how to model this aspect. Thank you for any ideas on this. Why you are ignoring contribution of other variables?",
    "what does: \"number of non-registered user rentals initiated\"mean, in simple words? So that you could try two different models to predict each of the variables and then sum them as a final prediction. I believe it refers to the number of people who rent a bike who don't already have a registered \"account\" with the bike share service, nor do they create one upon renting the bike. A registered user would be a frequent user: they have/create an account, and rent a bike frequently. A casual user would be an infrequent user: they rent a bike without having/creating an account - perhaps for a single afternoon when their car breaks down. There is no variables casual/registered in the test data. So our predictor can't use them explicitly.Why train data contain them?Thanks. You are right!Thanks for the quick replay.",
    "I am getting this error when I am calling predict.Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) :variable lengths differ (found for 'day')In addition: Warning message:'newdata' had 6493 rows but variables found have 10886 rowsNeed help to resolve this. How   do you  fixed  this   issue  , I have  the  same problem  when  trying to  use  ensemble learning?",
    "We have just released a tutorial of boosted decision trees using GraphLab Create which get's you very quickly into the 15th place on the  leaderboard: http://blog.graphlab.com/using-gradient-boosted-trees-to-predict-bike-sharing-demandThanks to Jay Gu from the GraphLab team for sharing! @lnoddy, it's possible you are misinterpreting the rules (they are a bit vague on this point). The rules actually state, \"You mustpredictthe total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period\" (emphasis added). They actually don't say that your models must bebuiltusing only information available prior to the rental period you are predicting. It's possible that the rules may be implying that when you predict the demand for, say \"2012-10-23 06:00:00\", that you cannot use the weather/windspeed/humidity/temp/etc recorded in the test set at time \"2012-10-23 07:00:00\" or thereafter. So maybe it's okay to build models using all the training data, as long as at prediction-time the models do not require future knowledge. Does that make sense?I'm not sure what the true intent of that 'rule' statement is. All I'm saying is that it is a bit vague and under one interpretation, the graphlab create solution may be quite valid. Illegal code which breaks the rule \"Your model should only use information which was available prior to the time for which it is forecasting.\"Still, it's a good example of code that can be used for contests that don't involve time series prediction.",
    "How I can calculate my accuracy before submitting the solution?I tried rmsle function but getting NA, can anyone help me here> rmsle(train.data$count, prediction)[1] NAWarning messages:1: In Ops.factor(1, predicted) : + not meaningful for factors2: In log(1 + actual) - log(1 + predicted) :longer object length is not a multiple of shorter object length The message says that two vectors are of a different length. What is the source data for your prediction - is it test or cross-validation sets?Check your length first:print(length(train$count))print(length(prediction))print(dim(prediction)) # in case this is not a vectorI am attaching full working example.You also need two files from here to run it:https://github.com/artem-fedosov/bike-sharing-demand/",
    "Hello  , I am getting  the following  error when  I submit  my file:ERROR: Unable to find 6493 required key values in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 00:00:00' in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 01:00:00' in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 02:00:00' in the 'datetime' column ERROR: Unable to find the required key value '2011-01-20 03:00:00' in the 'datetime' column ERROR:Any suggestion? Tnks Glad  it  was solved! FYI, even fixing the date format didn't get it to work for me using windows/excel.  I had to use notepad to convert to UTF-8.  That did the trick.-Jeff I solved it  tnks!",
    "Hi All,My name is Vineel,I am  Masters computer science student in the University of Utah . I would like to join an existing team. Please let me know, if you are looking for a team-mate. I am currently working at eBay on Text Classification.I would like to join an existing team.RegardsVineel Hi,I am interested to join the team as well. mail2kalyanji@gmail.com Hi everyone!!I'm also interested in joining the team. I  work on R.My email id is postmysubscriptions@gmail.comThanks!! Hi, This is Tanmay. I am a student at BITS Pilani. I also doesn't have a team.tanmay2893@gmail.com Hi this is Kunal from Bangalore,I would appreciate if you can add me to the team. I have experience in SAS & R.kunal.bhowmick87@gmail.com HiI am a 2014 graduate from IIT Kgp India and would love to work with some data science enthusiastic people .I am worked on multiple data science and optimization project during my course .Please mail me .id: binit1ag@gmail.comThanks Hi,I think I'm a little lost, sorry for that, but I did not receive any email of the request I did of joining this group. What do I have to do?Please help and thanks Hi,I think I'm a little lost, sorry for that, but I did not receive any email of the request I did of joining this group. What do I have to do?Please help and thanks Still have room for anyone else?Email: Bhavikpatel576@gmail.com Hi,I am interested to join the team. I come from MIS background, took some ML classes through MOOC, and work on DWH and BI.oberisklcw@gmail.comThank you! yes you can. Hi,Please add me to the team. I work on DWH and BI, interested in data science.tilakkumar.n@gmail.comThanksTilak Hi,My name is Niko Gamulin, I come from Slovenia and am looking to join a team.Currently I am pursuing a PhD from the field of machine learning and would be glad to exchange experiences with other people from the field. This is my first kaggle competition. Today I have tried to solve the problem by implementing linear regression in Matlab (https://github.com/nikogamulin/kaggle-bikes). Besides Java, Matlab, and python which I used so far to implement machine learning models, I have also experiences with other languages, such as C, C++, C#...My email: niko.gamulin at gmail.Niko Hello,I'm currently doing an MSc in Com/Sci and I'm quite busy, but I would love to join a team, contribute as much as I can and learn from others.Not sure how to join a team though? My email is kevinmcinerney8@gmail.com I too am interested in joining a team. I have successfully completed R Programming course on Coursera and currently taking Introduction to Data Science by UW.E-mail: akshatadawal@gmail.com Hi All, I interest too for join a Team, I have a good background in statistic, and I start to appiled machine learning. Thanks, my email is JChaudourne@gmail.com Of course. I will add u guys in the group later. Hi Henrilin28, I'm also looking to join a team so that i can get started. I have completed the courses on machine learning and R, and now i want to learn more. I'm also having the experience in big data techs like MapReduce, Pig Hive, Java, Unix. Can you please let me know if it is possible for me to join your team? My mail id is arunsharma.exe@gmail.com Hi,It'd be great if you accept me to join this team, I've finished successfully coursera Machine Learning (Andrew Ng) and Statistical Learning (T. Hastie, et.al).my email: ofigue@gmail.comBest Regards I would appreciate if you can add me to a team. I have experience in SAS & somewhat in predictive modeling. I would appreciate it you add people like in your team. Based on your bio's , this seems a very exciting team where I can learn a lot. (fhhh09 at gmail) Hi, I am raj, looking for a team to join Bike Sharing Demand,  Please add me raj4u4all@gmail.com Hi,I'm a Consultant at Infosys working with Analytics and R language. Have done certification in  Machine learning and data analysis.My email id : aggarwal.manik@gmail.comPlease add me. I would like to be a part of the team and participate in the competition. I can devote quite good amount of time to this. Will be the active member, just need some direction from senior folks.Manik Hi,I just add all of you in the group. Actually, the groupjust beginrecently.plzlet me know if there is any question.Henry Hi,Looking for a team to join. Hello,I have been self-studying statistical modeling in R for a few months and interested in machine learning by doing this project together with other peopleI am math/econ major at Pennemail id: jungyeom0213@gmail.com",
    "These are the rules as stated in https://www.kaggle.com/c/bike-sharing-demand/rules1. Due to the public nature of the data, this competition does not count towards Kaggle ranking points.2. We ask that you respect the spirit of the competition and do not cheat. You should not submit entries based on test-set answers or train your model on the test set. Hand labeling is also forbidden.3. Your model should only use information which was available prior to the time for which it is forecasting.Can we download and use external weather data (not just 1-4). For example, a more detailed level of precipitation?It is not stated weather this obeys the rules.",
    "*The following is not an original idea of mine - it was pointed out to me by another Kaggler.We are given data for Days 1-19 of each month, and asked to predict for the remaining days of each month. As such, we are more than able to use data for, say, February 1-19, 2011 to predict for January 20-31, 2011. Does this not flaw the contest entirely? cavaunpeu,Refer tothisthread.",
    "When I model on casual and registered users separately then the rmse are 20 and 36 respectively.result_casual = predict(gbmmod_casual, traintest,best.iter_casual)rmse(result_casual, traintest$casual)result_registered = predict(gbmmod_registered, traintest,best.iter_registered)rmse(result_registered, traintest$registered)To combine the predictions I'm using something like this:rmse(result_registered + result_casual, traintest$registered + traintest$casual)The result of this comes out to be ~47. This is almost the same as when predicting on count alone.Question is that since predicting on casual and registered differently has better results why the rmse increases when they are combined. Am I doing it wrongly?Note: rmse here is from hydroGOF library.",
    "I'm new to R but have some experience in Python (new to the scify library toolset).  Was wondering has anybody done this in Python using random forest generator?If not is there a reason why you would choose R over python?  Thanks. Hi Linh,I'm new to this but started with the Titanic competition, it has great tutorials for Numpy, Pandas and Scikit-Learn. Check it out - it may be exactly what you are looking for. Hi ViennaMike,Thanks for you reply. I really need to explore more on Scikit-Learn. If possoble, can you give me some pointer to help me quickly delve into this. Hopefully, it can help me improve my python solution for this problem.Thanks K32lv1n: I assume you're referring to Scikit-Learn as the toolset. No reason not to use it. You'll see a lot of R, python, and Matlab used on various competitions. I won't get back to working this one until September, but I'm planning to use python, built around Scikit-Learn and Pandas.Linh Duong: I'm not an R user, and from what I understand there are more R tools for pure analysis and ML, but with Scikit Learn (and perhaps Pandas) I'm not sure why you think summarizing or training the data would be tedious in python. Without these or similar libraries, sure it would be tedious, but with them it's not.  And most machine learning algorithms are part of Scikit-Learn (neural networks being a major exception). Honestly,We can use python to do this project. I am also trying to use MapReduce technique to analyze this problem.The advantage of R (over python) is we can have a lot of functions available to use, for example to summary the big data set or train the data set based on certain inputs. This will be a very tedious job in python.",
    "I split my training data into training data (80%) and test data (20%) so that I could validate my random forest approach without using up all of my precious daily Kaggle submissions.I then performed a pearson r correlation between my predicted counts and the actual counts taken from fake test_data I sliced out of the training data.However, when I added a bunch of features, although it did vastly improve my pearson co-efficient, it also drastically reduced my score on the leaderboard. So it's not a reliable measure.Back to the drawing board.Are there better ways of doing this? I currently have no way to benchmark my algorithms apart from submitting them which sucks. Don't forget, this is a time series. This \"classic\" 80-10 split of the data can mislead you. If you split the training data by \"day\" (train set where the day <=12, test set where the day >12 for example) column and use RMSLE, you will get reliable benchmark. Someone may find usefulrmslefunction fromMetricspackage.library('Metrics')rmsle(cross_validation$count, predictions$count)Returns required metric. Nice! Much appreciated. Using that I finally realized what I've been doing wrong :))) I used the rmsle function on the train data instead of test data (I did split my train data into \"actual train\" and \"test\" for cross validation, but I never used the test data). Now both my rmsle and that one are saying the exact same thing (and much closer to what Kaggle says).Thank you! That's what I use too, but it's way off in my case :(  Used to be OK'ish (within 0.2) but now got to within 0.7+ (I get ~0.3 locally, and ~1.0+ on Kaggle). Regardless of how I split the test data (50/50 or by day as suggested above). If you're still wondering how you might implement your benchmark, here's the code for what I did in R.  I created a function bikeevaluate and when I call it I pass my test data frame (split from the original train data) and the prediction vector.  I split my training data 50/50 into a training and testing set, and the benchmark function below has been giving me within 0.02 of the score I get on Kaggle when I run my models on the full training set.bikeevaluate <- function (data, pred) {return(sqrt(1/nrow(data)*sum((log(pred+1)-log(data$count+1))^2)))} Implement RMSLE, which is the evaluation function used by the leaderboard. Pearson r correlation measureslinearcorrelation. The evaluation function used by the leaderboard is on the log scale.You can think of this as wanting to minimize theorder of magnitudeof the error, rather than the actual error, or squared error. By implementing RMSLE and testing against that, you can get a better idea of how well you'll perform on the leaderboard.Ideally, your model should also aim to optimize for the log error as well, instead of squared error. Since random forests operate by discretely splitting the data in input space, random forest regression may be robust enough to do that on its own without you explicitly telling it (I would have to brush up on my random forest knowledge to verify this). But it may (or maybe not?) help to try to predict thelog-transformedcounts, and then transforming them back afterwards.",
    "Hi All,I am new to Machine learning and to kaggle. So excuse me if my question is dumb.I trained using the most simple model i.e Linear regression. In my first attempt I extracted only time from date time and trained the model with time and all the other attributes. I got an Adjusted R2 value as .63 and got the RMSLE on submission as .99When i extracted Year & Month also from datetime, i got Adjusted R2 value as .69 but after submission RMSLE score ~1.09Does this mean that my model is already over fitted (suffer from variance) ? I am little confused here. Hi Sujoy,The problem here is that this is not a typical problem you should be solving with Linear Regression. Just check out Time Series concepts as this is a time series problem. In time series you have two concepts- Seasonality and Trend. So keeping this in mind i assumed Trend to be constant for each month and seasonality to repeat every 24 hours and every 7 days. So a monday will be same as all mondays and 3:00 am will be similar to 3:00 am's. If you plot the the same you can actually see the trend.I used a very simple model as my first cut. Below are the details-Calculate the mean for every combination of month, hour, day_of_week and year from training setPredict as per the  combination of month, hour, day_of_week and year.So all Jaunary, 3:00 Am, Monday , 2011 will have one prediction which will be the mean of the similar combination in the training set.This very simple model gave me a score of .56372 and a ranking of around 200 (better than 50% of the people).  Now i will proceed to use more powerfull Time series methods to predict the seasonality and trend components separately which will definately better my scores. Hope this helps.RegardsGautam Thanks Gautam Linear Regression is actually very useful in this problem if used right",
    "Hi,I am having a problem using gbm, R code is:boost.data=gbm(registered~.,data=dataset[,c(2,3,4,5,6,7,8,9,10,12)],distribution=\"gaussian\", n.trees=6000, interaction.depth=4,shrinkage=0.2,verbose=F)and prediction,pred.boost1 = predict(boost.data,newdata=test[,2:10], n.trees=6000)but some of the predicted values are negative! How can this happen?, what am I doing wrong? any suggestion with the code?Thanks in advance.",
    "is the ground truth leaked somewhere? how come people can get perfect solution? What does 'Hand-labeling' mean? Yes, this is a public data set. Per the rules:Due to the public nature of the data, this competition does not count towards Kaggle ranking points.We ask that you respect the spirit of the competition and do not cheat. You should not submit entries based on test-set answers or train your model on the test set. Hand labeling is also forbidden.Your model should only use information which was available prior to the time for which it is forecasting.",
    "The data definition says Holiday: shows if the day is a holiday. But it doesnt explain what value in the column corresponds to what? Does the column say 0 -> not a holiday and 1 -> holiday?Likewise,Workingday: whether the day is neither a weekend nor holiday0 means day is working day and 1 means its either weekend or holiday? Is this better modeled as a single categorical feature with three possible values:\"Working Day\" (workingday=1, holiday=0),\"Holiday\" (workingday=0, holiday=1), and\"Weekend\" (workingday=0, holiday=0)....?Presumably there is no case where workingday=1 and holiday=1. If you look at the data, it becomes a little more clear.For example, July 4th has holiday=1. July 4th is a holiday, so therefore holiday=1 means that the day is a holiday.Similarly for working day, Jan 3rd, 2011 was a Monday, Jan 7th, 2011 was a Friday, and workingday=1 for Jan 3rd - Jan 7th, but is 0 on Jan 8th (Saturday). Therefore, workingday=1 means it is Mon-Fri.However, workingday also takes holidays into account. July 4th, 2011 is a Monday, but since it is a holiday (holiday=1), workingday=0.",
    "Hey all - I entered this competition purely for knowledge - I wanted to try a regression problem with seasonal and cyclical elements.  I'd like to start a discussion on the general approach people are taking to solve the problem.I'll start - here's the gist of what I've done so far:- Load the data- Extract the year, month, day-of-week and time-of-day data from the data/time stamp- Convert categorical data to binary features using sklearn OneHotEncoder.- Scale the scalar features using sklearn StandardScalar- Select the final features: (scalars) temp, atemp + (categorical features) weather, season, working day, holiday, year, month, dow, and hour broken out into binary features- Train two models: one for the casual rentals and one for the reserved rentals (I tried a number of different algorithms but settled on RandomForestRegressor tuned with GridSearchCV)- Score the models using RMSLE- Apply the models to the test data (prepared as above)This approach gets me to ~0.54 RMSLE on the leaderboard.I'm guessing my approach is overly simplistic and doesn't accurately account for the seasonality and cyclic elements (though I though including year, season, month, DOW and hr would have been enough).I also have a feeling I have a bug in my code around how I'm employing cross validation... I definitely get the feeling that the model is over-fit.If anyone is interested, my code can be found at: https://github.com/ecodan/kaggle-bikeLook forward to hearing how you approach(ed) the problem.Cheers,Dan I was trying to split the data into categories. For example, when I did a linear regression for the whole data, I observed that model was not upto the mark. But on splitting the model on hours and applying Linear regression, I got better R2 Value, which showed my model was getting better with its prediction.So, wondered if you used any of the clustering of data and applied the technique on each cluster of data separately like I did. I didn't try clustering... didn't even occur to me.  How would you apply a clustering algorithm here? Hi Dan, good to go through your approach. I have started with similar approach of data exploration and preparation by extracting month, year, hour and converting all continuous variable to categorical variable. Applied linear regression using SAS and got R2 Value of 44% for registered count and casual count R2 value stood at 19% (too less for my liking !! ). Looks like I need to do quite a bit of tuning. Trying decision tree with linear regression now.Did you try any clustering of data ??",
    "hi,I'm new to data mining, and this is my first kaggle competition (without tutorials).At first I tried build simple decision tree (as I'm newbe), which was fine. I extracted time from datatime column, and just used decision tree in R over it. Then I worked on decreasing overfitting, and got my best submission (0.53).At a next step I start to trying with RF algo, but I have much worse stats with it (around 0.8).And now I'm wondering - Is it me, or DT really works this better that RF? Is it possible, or I made a mistake?I used rpart, and randomForest in R for the same features. RF for ntree = 100 (score 0.77), ntree = 300 (score 0.82) ntree=1000 (score 0.83) more trees, worse score. Could someone explain that to me - what's happening? HiI am just learning about random forests myself so I'm not an expert, but I have a couple of suggestions that may help those who are finding worse results with a forest than with a single tree.  If anyone who is more familiar with forests thinks I'm misstating anything, please offer a critique.I'm working with the randomForest package in R, so these comments may or may not be relevant to python or other languages.I would try to tinker with the minimum node size.  The default for the randomForest package seems very aggressive, which I think is supposed to be ok because the voting should account for overfitting in individual trees, but I've found better results when I increase the minimum node size (don't increase too much though).I have also been adjusting the number of predictor variables used by each tree.  It seems like the defaults in the randomForest package are designed for datasets with many more predictors, so I increased the the number of predictors used.Again please take these comments with a grain of salt, but I hope they help. @Carter WangHere is my code in Python:dataPath = \"kaggle/201406-bike/data/\"outPath = \"kaggle/201406-bike/src/outputs/results/\"import pandas as pdfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.ensemble import ExtraTreesRegressorfrom sklearn.ensemble import RandomForestRegressordef loadData(datafile):return pd.read_csv(datafile)def splitDatetime(data):sub = pd.DataFrame(data.datetime.str.split(' ').tolist(), columns = \"date time\".split())date = pd.DataFrame(sub.date.str.split('-').tolist(), columns=\"year month day\".split())time = pd.DataFrame(sub.time.str.split(':').tolist(), columns = \"hour minute second\".split())data['year'] = date['year']data['month'] = date['month']data['day'] = date['day']data['hour'] = time['hour'].astype(int)return datadef createDecisionTree():est = DecisionTreeRegressor()return estdef createRandomForest():est = RandomForestRegressor(n_estimators=100)return estdef createExtraTree():est = ExtraTreesRegressor()return estdef predict(est, train, test, features, target):est.fit(train[features], train[target])with open(outPath + \"submission-randomforest.csv\", 'wb') as f:f.write(\"datetime,count\\n\")for index, value in enumerate(list(est.predict(test[features]))):f.write(\"%s,%s\\n\" % (test['datetime'].loc[index], int(value)))def main():train = loadData(dataPath + \"train.csv\")test = loadData(dataPath + \"test.csv\")train = splitDatetime(train)test = splitDatetime(test)target = 'count'features = [col for col in train.columns if col not in ['datetime', 'casual', 'registered', 'count']]est = createRandomForest()predict(est, train, test, features, target)if __name__ == \"__main__\":main() @sharanbabukI tested rf with ntree=100 versus ntree=2000 that I used before. It saves time dramatically and gives very close predictions. I think I will go ahead with ntree=100 setting while looking for better models.rf with ntree=100Time spent: 22.92656 secsrmsle on cross-validation set: 0.5581363rf with ntree=2000Time spent: 7.808788 minsrmsle on cross-validation set: 0.5551546 @sharanbabukI finally got to 0.58 with random forest (from caret package). Running time highly depend on number of features (as I understand it treats every factor level as a new feature). The one that gave me 0.58 computed in 37 minutes with doMC package (do multicore). I was monitoring CPU load and tuned number of processes to have close to 100% load (2 processes gave me 50-60% load on my 2 core CPU, so I used 5 processes instead). I do not have the exact combination of features I had then, present model takes 1.5h to compute.I am not happy with time it takes as I am not able to iterate quickly. I saw people claiming that they are able to get to 0.5 with linear regression models. I am in awe when I hear it. Because it means that I am missing some ingenuity in my models.There is also a big question for me - should I use factors or integers and why? I did not give myself a clear answer yet. I am sorry for the big piece of code I am publishing. I am rather upset as I got much higher error than you people did. The code is such simple that I feel comfortable publishing it. It gives me rmsle of 0.9155291. If I change factors to integers it gives me rmsle of 0.8580331. This cross validation set rmsle is close to one I get on public leader board. The error is far higher than what you guys are able to get.May be somebody can spot what I am doing wrong?library(Metrics)  # for beautiful rmsle error calculationlibrary(rpart)    # for decision tree algorithmlibrary(caret)    # for data partitioning (createDataPartition)set_up_features <- function(df) {df$datetime <- strptime(df$datetime, format=\"%Y-%m-%d %H:%M:%S\")df$hour <- as.factor(df$datetime$hour)df$wday <- as.factor(df$datetime$wday)df$month <- as.factor(df$datetime$mon)df$year <- as.factor(df$datetime$year + 1900)df}get_predictions <- function(fit, test) {Prediction <- predict(fit, test)Prediction[Prediction < 0 ] <- 0Prediction <- as.integer(Prediction)data.frame(datetime=strftime(test$datetime, format=\"%Y-%m-%d %H:%M:%S\"), count=Prediction)}train_raw <- read.csv(\"train.csv\",colClasses = c(\"character\", # datetime\"factor\", # season\"factor\", # holiday\"factor\", # workingday\"factor\", # weather\"numeric\", # temp\"numeric\", # atemp\"integer\", # humidity\"numeric\", # windspeed\"integer\", # casual\"integer\", # registered\"integer\" # count)train_raw <- set_up_features(train_raw)set.seed(415)inTrain <- createDataPartition(train_raw$count, p=0.7, list=F, times=1)train <- train_raw[inTrain, ]cv <- train_raw[-inTrain, ]dtfit <- rpart(count ~hour +month +year +weather +atemp +workingday +holiday +windspeed +humidity +season,data=train)print(rmsle(cv$count, get_predictions(dtfit, cv)$count)) Thanks Wang .. i just realized that ... thanks now its working fine..Vikas GoyalIndia Why is your outcome variable datetime? The variable on the left of \"~\" should be what you're trying to predict. Hello Guys,I am trying single tree but facing memory issue... i am on a 32 bit Win7 with 4 gb RAM...I have converted datetime to POSIXct and season, holiday, workingday and weather into factors and then tryingcycletree=rpart(datetime~.,method=\"class\",data=train1,control=rpart.control(minbucket=25))and i am getting memory error..m i doing something wrong ...Please share..VikasIndia HiI have the similar results. A single tree in R works much better than random forest. I am also confused. Has anyone tried with M5P(M5Base) algorithm ? It Implements base routines for generating M5 Model trees and rules. M5P learns a \"model\" tree - this is a decision tree with linear regression functions at the leaves. It can be used to predict a numeric target (class) attribute. It produces a piecewise linear fit to the target. Unfortunately I lost my code for a single tree, but I remember that 2 things was crucial for my work.1st, is to extract time from datatime column, and use it in DC. Time (hh:mm:ss) is the most valuable variable in my model.2nd, I tried to play with minsplit setting in rpart function (in R, package \"rpart\") and it worked well. I won't tell you exact value which worked the best for me, becouse I don't want to spoil all fun for you. Just try to build accurate model, but not too accurate - you don't want to overfit.I can try to code it one more time and show it to you, but i don't know if you want it/need it (my tree isn't that good anyway). Would anybody here like to share their inputs / settings for a single decision tree? I've tried Python and R and haven't gotten a score for a single decision tree under ~1.2. I've tried accounting for the hour within the day and the days since the first day (to account any potential increase in usage), but neither have shown to be of much use.Thanks. That's why I'm concerned about my results. I know, that not always more complicated model is better, especially in DM, but 0.53 with decision tree compared to 0.8 with random forest seems to be little odd. Maybe I'm making some sort of mistake with implementation. Random forest is an ensemble of decision tree. So It will give you better results than decision tree in most of the cases. i tried random forest, my best score was 0.4809; while decision tree was 0.56228But I did not use R, instead, my program was written in python.",
    "Is it correct that \"season\" = 1 when the month is January, February or March? The data description indicates that 'season'=1 corresponds to spring. Those season labels must be in error.  The description for the problem said this bike sharing program was for Wash DC. Having lived there in the past, I can tell you that Jan-March is DEFINITELY not spring -- can be quite wintry indeed.  So 1 should be winter, 2 spring.... This shouldn't really matter though unless you are adjusting your predictions by hand.  The labels in and of themselves, if treated as categorical variables in a ML algorithm, are arbitrary anyways (call season 1 Fred if you feel so inclined, and you should get the same predictions). I have observed a similar pattern. It seems that out of the 4 seasons, Spring shows the lowest average bikeshare count. Could it have something to do with the fact that the roads are slippery as the ice has only begun to melt? Thanks for sharing your code--I'm new to R.  The data field description indicates that season = 1 = spring.  I used espanarey's code from another thread to plot the average hourly number of riders on workdays by season (attached).   season =1 has the lowest average number of riders.  That doesn't seem correct if season = 1 = spring.  But it seems ok if season = 1 = January-March. The answer is yes. Here is how you can go about retrieving that.First, familiarize yourself with the following R commands: weekdays, months, seasons.If you wish to get the seasons from the 'datetime' variable provided it's as simple as:data$season1 <- quarters(as.Date(data$datetime, format=\"%m/%d/%Y\"))From here you can cross check it with the 'seasons' variable given in the dataset by:table(data$season1, data$season)This should give you what you wanted!EDIT: You could also check the months by doing:data$months<-months(as.Date(data$datetime, format=\"%m/%d/%Y\"))table(data$months, data$season)",
    "I took tree-based regression, it performed well. Since datetime is autocorrelation, it there anyone try the time series to predict the trends? @Novice: I did notice there are some missing hours in the test data on specific days.  E.g. on 1/26/2011 the hours only go up to 5pm and the next day they start at 4pm.  If you are specifically relying on the time data to be contiguous, then I see this is a problem.From my perspective, I've mostly ignored the sequencing of time and used factors for weekday & month.  Ignoring the sequencing of time means I don't have to interpolate missing values and I can randomly sample the dataset to create training and validation sets.  Here's how I've massaged the data prior to fitting a model...# Import training and testing datatrain = read.csv(\"train.csv\")test = read.csv(\"test.csv\")# Add dummy values to test dataframetest$casual = 0test$registered = 0test$count = 0# Bind train and test data togethercdata = rbind(train, test)# Convert some features to factorscdata$season = as.factor(cdata$season)cdata$holiday = as.factor(cdata$holiday)cdata$workingday = as.factor(cdata$workingday)cdata$weather = as.factor(cdata$weather)# Extract hour, weekday, month, and year from datetimedatetime = as.POSIXlt(cdata$datetime)hour = datetime$hourweekday = as.factor(datetime$wday)month = as.factor(datetime$mon)year = 1900 + datetime$yearcdata$datetime = datetime# Add the new features to the combined dataframecdata = cbind(cdata, hour, weekday, month, year)# Split in the corresponding train/test datasetstrain = cdata[0:10886,]test = cdata[10887:17379,] HI!I have also tried RF and GBM. GBM gave slightly better results than RF but it is important to avoid overfitting the models ..... I use random Forest in RAnd I separate the train data into two parts.rf_model_weekday = randomForest(count ~ . , data=train[train$workingday==1])rf_model_weekend = randomForest(count ~ . , data=train[train$workingday==0])This give me better result than not separating the data. There are 2 years in the data: 2011 and 2012. I think extra trees could distinguish this difference. Same as the month.If it could not work for GBM, I am wondering it's because of the core ideas between GBM and extra trees/random forest. I tried taking the most recent available past value from 7 days back (so same day of week, same time) as a naive 1st cut.  Surprisingly, this did slightly WORSE than the mean value benchmark. That surprised me. I know this would leave out temperature, weather, and holidays, and lag on seasonal shifts, but I still thought that with capturing the diurnal cycle and some trending in time it would have performed better. I welcome thoughts on why it didn't. I am using SAS. Trying with Linear Regression with Decision Tree. As of now, successful only in getting till Adj R2 value of 28%,which means I am too far from creating a good model for this.Now got to dig into decision tree and do some learning before proceeding further.Does anyone know if we can try Random Forest with SAS? I dont have any idea about Random Forest, hence please pardon me if my question was silly. Im using R for this.Apart from basic cleanup tasks (for example, fixing some of the atemp values), I cretaed a script which extracts the hourly weather data (e.g. precipitation,windspeed,temp etc..etc..) for 2011 and 2012 from a Washington DC weather station. I used this data to create a variable which gives a better reflection of hourly overall conditions than the existing \"weather\" variable and found that this improved my ranking.I have a created a model that creates an RF on casual/registered split by year and then simply adds them up. I've spent most of my time on feature engineering and next to none on actually tuning the model. I'm pretty new to this so still getting my head around the different tuning factors.Is anyone using anything other then randomForest? If so, what is it and would you be able to provide a link to a basic introduction ? Hi,I read that decision trees and Random Forest are being used, the only way to use them is by discretizing feature registered or casual. The question is how the discretization had been done?Thanks in advance @Matt the man: Yes I am referring to test data. How did you coerced weather in test data. ? and how about atemp and humidity ? did you use atemp and humidity at all ? if yes how did you fill these for test data ?can anyone please share their code in R ? @Novice - Are you saying there is missing data in the Test data?Like most people in this thread I'm using a tree based approached.  I have found my best results using Bagging.  Prior to fitting the model (using R) I coerced season, weather, workingday, and holiday as factors, and extracted hour, day of week, month, and year from datetime.  I'm using factors for day of week and month.  I dropped datetime, casual, and registered from the model.Has anyone used casual & registered?  I attempted to predict both of these and sum them to get the count, but this increased the MSE I've been using to validate my models. How did you guys fill in the data for atemp, weather etc. in Test data ?For atemp and humidity I calculated the daily temp by (Temp on 1st of following month - Temp on 19th)*(Date-19)/11. Here I assumed a uniform increase/decrease in temperature. Similarly for humidity.But I am struggling If should even consider weather ?? also should we even consider Date in our training data?? @Kelly ChanI use R.These two models give me around 0.5Big errors mainly occur between hour 0 to hour 5. @oncemoreI separated two datasets to train, but, unfortunately, it did not improve my score. I used python. Blue Ocean wroteHave you used any extra features except the additional ones such as Year, Hr, dayof week ,monthYes all of these extra features derived from the field \"datetime\" ! @Toulouse,Have you used any extra features except the additional ones such as Year, Hr, dayof week ,month .When I use month though, the RMSLE decreases.CheersBlue Ocean I tried Random Forest also; however, it gave me a worse prediction than the Decision Tree. My best model is from decision tree and I did not engineer any fancy feature yet, beside the hour of the day.I also tried to add in the weekday as new variable. It worsen the model actually. Thanks Kelly.It would be nice to hear from others about the feature engineering and the algorithms , data visualization.Would be very useful if the Leaders and Master Kagglers provide their 2 cents .CheersBlue Ocean Thanks Kelly; sorry I had used day ; \" I had named the column weekday\"I just used library(gbm) in RWhen I had added month, the accuracy decreased.Would year be useful since all of the data is in the same year. After splitting the datetime as year, month, day, hour, i applied extra tree with all features, it worked well a little bit. I did not extract weekday, because working day and holiday have been existing.May I know which library of GBM did you use? How about neural network? For this prediction, there was a little difference - the first 19 days in the training set while the 20th to the end of month in the test set.What I am curious is how you constructed the program to fill in the gaps? I tried randomForest algo; it has worked OK.Anybody tried anything else ?We discussed about GBM in one of the threads ; and due to my inability to tune / add more variables , GBM has not performed well.Anybody tried doing anything with other algorithms such as time series algorithms ?Features used : weekday , hrFeatures removed : temp since atemp is there.Its encouraging to see many Kaggle Masters in the competition; it would be nice to know their thoughts on algorithms amd features.CheersBlue Ocean. I know, that's what I've worked on tonight, and it's much better. But I still thought just extracting the long-term trend, never mind the diurnal variation, would do better than just the flat mean. Had you predicted the trends by hours? Except the date, the trends in 24 hours are quite different, the shape is a wave.",
    "Did anybody try out GBM ? How did it performCheers,Blue Ocean I use GBM in R.set.seed(1111)genmod<-gbm(train$count~.,data=train[,-c(9,10,11)] ## registered,casual,count columns,var.monotone=NULL,distribution=\"gaussian\",n.trees=1200,shrinkage=0.1,interaction.depth=3,bag.fraction = 0.5,train.fraction = 1,n.minobsinnode = 10,cv.folds =10,keep.data=TRUE,verbose=TRUE)best.iter <- gbm.perf(genmod,method=\"cv\") ##the best iteration numberpred = predict(genmod, test,best.iter,type=\"response\")It is a very baseline GBM, it  gave about  0.55The difference between 0.43 and 0.55 => variable selection and transformation. I use it, perform well. Hi Carter Wang,I have tried the code in R, but RMSLE for tarining Data is 1.28 and leader board 1.30!How can I improve the results?Thanks in advance razgon wroteI use GBM in R.set.seed(1111)genmod<-gbm(train$count~.,data=train[,-c(9,10,11)] ## registered,casual,count columns,var.monotone=NULL,distribution=\"gaussian\",n.trees=1200,shrinkage=0.1,interaction.depth=3,bag.fraction = 0.5,train.fraction = 1,n.minobsinnode = 10,cv.folds =10,keep.data=TRUE,verbose=TRUE)best.iter <- gbm.perf(genmod,method=\"cv\") ##the best iteration numberpred = predict(genmod, test,best.iter,type=\"response\")It is a very baseline GBM, it  gave about  0.55The difference between 0.43 and 0.55 => variable selection and transformation.Hi, could you let me know what kind of variable selection and transformation you have made? Thank you in advance... Following the code (as best as I can) I get stuck with a score of ~ 1.2. No idea what's causing it.---set.seed(1111)genmod<-gbm(train$counts~.,data=train[,-c(1,10,11,12)] ## registered,casual,count columns,var.monotone=NULL,distribution=\"gaussian\",n.trees=1200,shrinkage=0.1,interaction.depth=3,bag.fraction = 0.5,train.fraction = 1,n.minobsinnode = 10,cv.folds =10,keep.data=TRUE,verbose=TRUE)best.iter <- gbm.perf(genmod,method=\"cv\") ##the best iteration numbertrain$pred = predict(genmod, train, best.iter,type=\"response\")--- Yes;Also you may get negative values for predictions ; you may change it to absolute value Is this means exclude column 1,9,10,11 ? one changedata=train[,-c(1,9,10,11)] ; @Carter WangThanks for your answering. Do you have any idea to avoid this situation ? I just followed the code that @razgon posted. I think that means that you're considering each hour as a separate variable instead of considering the time or hour to be continuous or by considering 24 categorical variables. You get an error because there are 10886 different hours in the training set and it limits categorical variables to 1024. I'm not sure how everybody here handles it or exactly how R handles it, but I imagine that's your initial problem. I tired GBM  but I got following error\"4 nodes produced errors; first error: gbm does not currently handle categorical variables with more than 1024 levels. Variable 1: datetime has 10886 levels.\"Can anyone help me ? Thats correct hedgehog.But therefore this would be incorrect prediction; so you would set it to zero ; Is this a correct statement ?CheersBlueOcean @Blue OceanLook at the vector of predictions,  there are some negatives values occur. That's why you've got NaN. I tried using it.However one changedata=train[,-c(1,9,10,11)] ;Did you also get the date column out of the train dataset ?When I calculate RMSLE, it gives me \"In log(predictions + 1) : NaNs produced\".Am I missing something ?Cheers,Blue Ocean @razgon,I did not get impressive results with gbm with ntrees = 1000.Being a novice, when I tried the tuning parameters with caret, some of the predictions were negative. I must be doing something wrong.",
    "In the top-left of the leaderboard, it's written that the leaderboard is calculated on all of the test data. This implies that the public and private leaderboards are identical - correct? If so, why, then, is there still a form for selecting your submissions to be evaluated on the final, private leaderboard when the competition comes to a close? There is no real concept of a private score here - 100% of the test set is in the public fold. So, for my own edification, the score currently shown on the public leaderboard is in fact the same as that on the private? Great question. Right now we use the same code to handle all competitions, regardless of how much sense it makes with respect to the public/private split. You can ignore it and hopefully it'll be changed in a future release.",
    "Participants in this contest may be interested in the paper \"The Impact of Weather Conditions on Capital Bikeshare Trips.\" See also http://mobilitylab.org/2014/01/27/explore-the-links-between-weather-and-capital-bikeshare-ridership/ Thank you very much.I just looking for some papers.",
    "For anyone interested in playing with CaBi data and sharing cool apps or data visualizations, we'd love to have you join us at ourTransportation Techiesmeetup group. See http://www.meetup.com/Transportation-Techies/events/189683512/ - next CaBi Hack Night is August 7.You can see a wrapup of the last cabihacknight on theMobilityLabsite.",
    "I have been experimenting with the Analyze and Data Mining Tools with Excel Data Mining Add IN, while running SQL Server in the background of course.Would someone be able to tell me which specific item(s) I could use to predict a final number for each row of data in the test model after running which? analyses on the training model.I have already run some clustering and regression analysis to study the data. But how do I preidict a \"specific\" value (not just a cluster group) for the \"count rentals\" column.Thanks",
    "What's the mean value Benchmark? Just the mean across all days and hours applied to each entry, or something else? Nevermind, yes, it is.",
    "Hello  ,I   don't  know  if  I am  allowed  to paste code here   , but  since   it is  an  knowledge competition  I suppose it is  ok  .I  am having   an issue  with  a  neural  net   that I  tried   to use  for this  competition   in  R. More exactly the data of the  problem  are   described bellow:What I did :install.packages('neuralnet')library(\"neuralnet\")#read the training datadatatrain=read.table(\"train.csv\",header=F,sep=\",\",skip=1)cnames=readLines(\"train.csv\",1)cnames=strsplit(cnames,\",\",fixed=T)names(datatrain)=make.names(cnames[[1]])#train the neural net ( 8 neurons)neural.train=neuralnet(count~season+holiday+workingday+weather+temp+atemp+humidity+windspeed,datatrain,hidden=8,threshold=0.01,stepmax=10000)#After that I read the test data and computed:predictCount=compute(neural.train,test)where test was the variable wich stored the values from test data .I tried for feware variables and I got an huge error , something is not working corectMaybe someone  with   experience  in  R   would  have  some  suggestion regarding this issue Ps: I also   write  wrong the  tile   instead of  R  I wrote Ri   and  I   don't kno w   how to    correct it."
]